{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inverted_index_creataion_and_ranks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naama133/WIKI_IR_ENGINE/blob/master/inverted_index_creataion_and_ranks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#todo - delete this section before submission - those are the docs ID's for each given query in the train set\n",
        "docsID = [5652480, 5382150, 39813131, 50683920, 65732625, 286747, 147484, 180251, 5046302, 30810141, 925728, 25935906, 8912934, 925736, 925740, 8773677, 40075315, 61513780, 10887219, 7151675, 3735620, 565318, 63602774, 196698, 3473503, 188521, 55115883, 21381229, 1015919, 49127538, 254067, 7913597, 1179787, 66355340, 19988623, 311440, 1540241, 483473, 1884312, 90267, 26763420, 28754077, 1491100, 6168739, 53354661, 64192678, 46186668, 15179951, 57393329, 196789, 66846904, 35250361, 6815937, 49355, 20775117, 344269, 2203853, 7061714, 7602386, 26591446, 57559, 18948312, 319703, 48308444, 1655005, 344293, 8175846, 21496038, 59539691, 762092, 55935213, 23732462, 48972018, 6709491, 1761526, 16982268, 65601792, 45482242, 25215235, 213251, 7381258, 22757645, 52076814, 7438610, 60088597, 51552534, 1360154, 3596573, 28688670, 6422823, 3965223, 66814247, 8495, 59162931, 24019253, 704826, 56598843, 442684, 17031486, 827717, 426315, 21078348, 29565262, 50577743, 2810191, 33538385, 360789, 21496150, 51478870, 360797, 38904159, 49185121, 5775715, 835946, 22503790, 1130867, 8094067, 16671094, 42549636, 5087621, 696712, 8806795, 2589070, 32276880, 12673434, 33178, 37061023, 4694434, 9675171, 246185, 5038510, 4055486, 1442241, 46784964, 1343949, 20439501, 7152087, 30876121, 516570, 37331423, 2040288, 1032674, 56721897, 40116717, 11583987, 40845811, 9511414, 3080697, 2564605, 2425344, 16899, 1516039, 57866, 33038861, 991758, 11231759, 49681, 606737, 164376, 33849881, 426522, 6939163, 4481567, 344611, 43098662, 61932070, 459304, 7529001, 21905962, 8970797, 19718702, 672302, 44270128, 33481262, 65176113, 46719542, 254519, 16990777, 21611071, 28271176, 22438472, 3039817, 16368206, 3252816, 3383895, 57947, 7660130, 10519138, 27837030, 34292335, 30597745, 1999475, 311935, 2474625, 39797382, 52077192, 8846, 2032271, 59540110, 22905489, 11322015, 48530084, 828085, 28844729, 524987, 5063356, 1360573, 53035710, 2933438, 2269888, 484030, 42951365, 52454088, 19931851, 23159504, 65028817, 1188562, 14754518, 31728348, 533214, 2581217, 33768164, 697063, 1950442, 44008170, 32613098, 746225, 57672434, 19079925, 43959031, 45359871, 5686025, 37593868, 3293969, 64594706, 1999636, 33039125, 7209753, 43655965, 31425310, 705310, 60769053, 10715937, 60744481, 475952, 23446323, 992052, 3883838, 12174150, 3720007, 23454546, 18383702, 7971671, 566114, 1221476, 47489893, 43344741, 181094, 29156200, 18408298, 8250222, 5522290, 1811320, 30294906, 13280124, 4146044, 20505468, 6972293, 19702661, 17621896, 37593992, 11486091, 246674, 877461, 4301719, 39322520, 15434651, 910244, 23954341, 24224679, 40428462, 5563310, 33407925, 1532857, 1811390, 60670913, 17785794, 4432842, 13902799, 23487440, 1975250, 5866454, 19776474, 6693851, 7439323, 304091, 1844187, 1197035, 58582001, 263154, 26952693, 65217527, 66554, 492540, 20071428, 36643850, 2941963, 25613, 1278990, 25614, 9232, 8848402, 31728660, 42812440, 4539416, 6497307, 56042524, 59917348, 24593449, 33637422, 7218230, 46269496, 53748793, 55411770, 222266, 295993, 582735, 738384, 746577, 83024, 27944015, 20874323, 44254295, 443480, 83036, 60851303, 49931371, 40748148, 1279101, 11216001, 57312392, 27051151, 926863, 74910, 672934, 42149032, 51578025, 52708523, 47350955, 263344, 1017009, 13051060, 418999, 1148089, 2720954, 419015, 61285582, 4465875, 705756, 55207134, 56313056, 18384111, 3450096, 1025265, 29066482, 9457, 9651444, 42223, 51430647, 59868408, 1762557, 30876926, 13624574, 20038918, 4105482, 42253, 65766671, 173332, 60310807, 55633178, 222491, 34080, 435492, 7775525, 4023588, 17720623, 4465969, 9315633, 25908, 4080953, 230716, 36906301, 2385217, 40068424, 1770825, 34964813, 1181008, 845137, 10691928, 55960921, 468313, 4523354, 7800160, 1385825, 13690215, 58731, 14181749, 31655286, 664951, 2794874, 31556991, 10265984, 43607423, 148869, 9110929, 34735506, 1041815, 1262999, 3188121, 9733533, 1533342, 36742560, 21579170, 52766120, 4875689, 42444204, 39585214, 1492414, 18621887, 976322, 28190146, 394697, 28771786, 34258, 36881878, 75222, 6931929, 1566175, 1074657, 6727140, 42411494, 46613990, 36619752, 54863336, 13370873, 1549819, 14001660, 24315397, 198153, 271890, 48317971, 99860, 51078678, 1730071, 75290, 43419164, 2565663, 31663650, 32400932, 3294758, 9397801, 7235116, 5498413, 550448, 6956596, 39675445, 427577, 41731654, 312903, 288328, 3868233, 976456, 35292743, 5350984, 5318218, 1156703, 1517154, 689763, 7718500, 3196517, 50795, 99951, 2295417, 10782332, 46302846, 149124, 62342792, 362124, 66414222, 59024, 1566357, 40797848, 3098266, 16795291, 255645, 3466910, 657055, 17868447, 48653985, 28026533, 2606761, 2500271, 22513329, 18745015, 509624, 599738, 509628, 23045823, 3286723, 12101316, 36439749, 2246339, 29828807, 26584776, 22144721, 53356243, 141013, 27289304, 1050340, 1435365, 4876011, 5302000, 1984246, 47638279, 10000, 3606300, 83742, 1435428, 7702313, 34113322, 509738, 1525546, 1722156, 42801, 23054130, 591668, 56731447, 8464191, 1857, 26879832, 1124192, 1959777, 9807715, 149354, 6154091, 39217006, 11495285, 40478583, 18745215, 2738057, 903052, 64210828, 2385806, 337814, 65693607, 264104, 63883178, 3631020, 346029, 56313776, 149426, 2541491, 6719413, 33367993, 32843712, 1411016, 34121672, 41609161, 47769544, 157649, 149463, 21604314, 53487581, 22144990, 57944031, 48736239, 43853813, 296950, 35973111, 5818361, 67528700, 20948989, 501757, 60483582, 59400, 11323402, 903187, 27363348, 305175, 19433498, 2074655, 1902624, 2213921, 2080, 5089316, 66938917, 13338662, 6424614, 886827, 763947, 1148978, 59516988, 26685, 92222, 57772096, 878659, 25045060, 1632324, 15190087, 66259016, 878666, 32639051, 42960975, 65366095, 52971602, 57591892, 1943640, 37652572, 6613094, 55511147, 55511148, 952429, 13936750, 1558639, 34900078, 55511155, 1402997, 42174581, 2175, 403585, 50276487, 11266183, 60500106, 56412300, 21899417, 632990, 3524766, 600236, 1493164, 2386093, 723126, 37882040, 47892671, 9586885, 403653, 58534087, 16943307, 3188952, 42846434, 64784613, 32016617, 559339, 42674415, 58255600, 27511028, 34482422, 35334391, 50727159, 559356, 48449792, 45443335, 56789255, 28125449, 5204237, 76046, 56289553, 84241, 40536338, 272659, 30853394, 1673495, 47778072, 7424280, 3533082, 100640, 56289572, 11921706, 14985517, 43911472, 58255666, 936243, 17402165, 36579642, 46745914, 17664319, 5048642, 3057990, 66029904, 26700112, 67922, 40438099, 9759063, 7301470, 10591, 37980517, 100710, 59591015, 346470, 6383976, 13502823, 43376, 61073786, 24963451, 8915322, 51582, 62482816, 5958023, 56289672, 35326347, 25659792, 575890, 18838, 879000, 22735258, 42469790, 35211682, 13412771, 5818788, 49891754, 47270314, 3664299, 3828139, 22809006, 66963891, 12003767, 55052729, 26880450, 133574, 47155656, 25061839, 52840911, 36891093, 64063961, 54077917, 28486111, 1591777, 43494, 20072937, 37669357, 41757168, 59892, 731640, 17566205, 18942, 36579839, 1878531, 739855, 31156754, 240148, 62851606, 51112472, 10619416, 17418777, 67414554, 6146589, 10922531, 48695845, 63351334, 18491946, 6154795, 2337323, 37497391, 2386481, 3738164, 592436, 412214, 1141303, 6523448, 24111672, 52554299, 50653758, 3746367, 821829, 11184711, 59988, 51800, 17107550, 48253536, 23030371, 371301, 37849704, 297576, 100970, 2968175, 2009711, 14174833, 6933106, 633458, 47827570, 47827574, 39774839, 371327, 24193668, 34843274, 58067594, 61893262, 1632912, 6908561, 11971218, 46426771, 322197, 1632919, 11061915, 11487904, 469664, 322210, 10916, 57313961, 7154355, 14650035, 43707, 47770304, 60992198, 50498250, 37243595, 38464202, 4344526, 2247376, 3287760, 26585811, 3115732, 264917, 19160, 27306717, 20302560, 40790752, 142049, 20089569, 8948452, 17296107, 2730731, 43617005, 33065713, 3107577, 11002, 805626, 7105280, 24963841, 26143506, 699156, 49998617, 5376796, 60705564, 666420, 2403126, 51505979, 5376827, 12487489, 502593, 11971397, 412488, 38153033, 52042, 265033, 28887886, 4819795, 3328852, 29403992, 26078050, 912227, 5376868, 568164, 42347365, 1248103, 47434601, 55536490, 9603954, 150389, 12209015, 1583998, 10062721, 2190209, 30796675, 1584013, 9874319, 54840207, 40774543, 60754840, 8571805, 31468446, 3214240, 43937, 5065635, 56363941, 3206057, 36662188, 76723, 846772, 31452088, 347068, 19390, 494530, 2698183, 50777038, 62557135, 6458321, 42757076, 224220, 412643, 1207268, 33680356, 6974438, 2132969, 13323241, 1403888, 36310013, 54537218, 48253954, 1330188, 10300434, 2575380, 53087254, 11286, 240668, 756765, 2550824, 66423851, 35392555, 1330224, 11313, 3378227, 2509885, 16591942, 5549127, 863306, 43977806, 41815118, 1092688, 142417, 21179478, 937047, 14109784, 5123174, 52328, 2608233, 29772915, 6245494, 22670461, 5270659, 19696772, 13315204, 10873990, 25767045, 1027207, 1485962, 62549131, 10218640, 17452177, 19620, 6311077, 41905327, 314544, 48286897, 560306, 21490868, 54406331, 978107, 7957691, 142528, 3902658, 617668, 60468426, 65785040, 39562449, 1273047, 27159781, 65785063, 1527015, 8277225, 4975851, 25496814, 3493102, 19701, 298232, 609528, 55225594, 24612090, 10448121, 2297090, 36875530, 43379979, 19344654, 8817935, 56069398, 249116, 3673376, 35917094, 26275110, 49278248, 240940, 773423, 10554676, 4730170, 1092923, 978236, 29437250, 38980932, 36164, 224584, 10669385, 9547083, 1060171, 1764686, 13200719, 59116880, 3190097, 12356944, 879962, 3419, 462173, 240989, 2051423, 37670240, 44133735, 2108776, 3280233, 1969515, 60583278, 52882803, 871796, 59534710, 12234103, 14953848, 48475515, 1125761, 2837891, 1428872, 2387344, 64695698, 1125781, 20327832, 19508643, 63327652, 19828134, 35270055, 49778089, 32468394, 1387946, 19886, 4148655, 63016369, 175537, 18353587, 35892659, 62705077, 4992439, 55864774, 4206029, 17345999, 650704, 33328593, 60883, 14945749, 1265113, 44674524, 4427231, 12119521, 60903, 60904, 978408, 27471338, 11193835, 12897776, 31591921, 519667, 10104316, 47615485, 63704575, 63090179, 44547, 3960323, 63090183, 8957449, 17255946, 2076169, 32255505, 39390739, 44568, 85533, 1257003, 4189740, 4451883, 44240443, 37989952, 2002497, 49065540, 64269900, 994892, 28239, 28240, 23678545, 11832915, 37400147, 5344857, 38342244, 11415141, 4992612, 35237477, 23834216, 66293350, 38833779, 1494648, 896634, 16969340, 3108484, 31919750, 53653129, 806538, 16887435, 13700748, 11921, 4361873, 20827796, 1044117, 85655, 8220315, 167585, 1765026, 3746, 396962, 2231973, 28327, 52906, 4673201, 487090, 3067569, 11882165, 18550455, 31526583, 7343803, 1068736, 8588996, 24997576, 14208718, 11984, 67759831, 64057049, 487132, 12070623, 1068768, 10079967, 33279717, 49458922, 43003632, 28086000, 55840499, 58150649, 1249019, 44035836, 372478, 9785087, 25390847, 60616450, 20229, 315141, 3845894, 44808, 1412877, 61402898, 60862228, 151327, 1093416, 64302888, 10669865, 44494636, 1027896, 159546, 315204, 184136, 37179209, 67071821, 51990351, 3921, 1101651, 22114132, 35458904, 8703840, 23269223, 2068329, 60534633, 1421165, 1339248, 20156275, 200563, 18911094, 2142071, 47837049, 21491579, 7368588, 21434255, 241559, 22294424, 36753304, 59502488, 39817115, 64499620, 60952488, 46567337, 61329320, 17362858, 2248622, 57069491, 430008, 61371, 40030145, 24997830, 12232, 47419336, 60698570, 348111, 49926096, 585681, 167891, 32362461, 4567010, 3502051, 28643, 54595558, 20455, 46665704, 479210, 41668588, 28848113, 1265651, 872439, 864249, 5763069, 42471422, 249858, 7139331, 64114692, 40783886, 51097615, 233488, 987153, 143377, 9433102, 63164437, 43601944, 15003673, 2387993, 15101979, 5132315, 2682907, 626718, 200734, 823320, 19542049, 1732642, 6393894, 1577008, 11980851, 2945076, 708662, 21106742, 28732, 57143357, 634940, 42586177, 60395596, 3403854, 18985040, 37728339, 1847385, 57159776, 18509922, 43569250, 67063906, 36969, 34336876, 3133549, 67063919, 16224368, 19492975, 2199665, 2052211, 49647732, 2650227, 64966775, 1929338, 60805249, 19558531, 2240648, 667785, 58708106, 1994895, 12431, 4649105, 8867983, 18968719, 610452, 56643734, 18944151, 217241, 61386909, 64639133, 17412253, 21549213, 25211041, 49770662, 2298022, 60739751, 3182759, 1814699, 1118379, 59568301, 3223727, 63295665, 856246, 63631542, 26775738, 55775421, 4468939, 46493906, 6050003, 10154200, 53469, 15003874, 54284514, 1183972, 17461478, 1822951, 18952424, 17740009, 6263016, 2732267, 11505904, 7852272, 35729649, 7778546, 299250, 66859257, 12652799, 9974017, 69893, 61583621, 36991237, 64704775, 11858188, 61624594, 44822802, 6672660, 5239060, 5230872, 34083096, 5083422, 168223, 24056098, 65499427, 2052387, 66605355, 2797868, 25219375, 37168, 3141956, 6984004, 64057670, 39293265, 50278739, 348499, 8474963, 2134361, 46207323, 36794719, 2117988, 20840, 618856, 68186473, 59052395, 67072363, 160108, 19124595, 63148404, 9195892, 49090939, 422269, 340356, 250247, 151944, 4489, 1421709, 54415758, 446865, 1765779, 15880608, 2453923, 58847654, 504230, 872876, 1315248, 21000624, 356785, 2199987, 2380213, 12915125, 340407, 1896889, 14438844, 30224834, 8892877, 26726864, 7082459, 15716827, 53866975, 12772, 602599, 70129, 60199413, 59519477, 14684661, 70136, 3322361, 29178, 4141563, 46551547, 6558203, 43250171, 67162622, 40759810, 68489731, 19288592, 19960341, 60535326, 18952739, 61592102, 4108839, 41546279, 21035, 8491563, 37421, 1765938, 40055348, 3371574, 59863622, 4681, 50565707, 987724, 23941708, 225870, 37454, 49132107, 12776016, 1471058, 10687051, 36835927, 5321303, 22893145, 67736156, 37469, 1946204, 32739936, 40956516, 3461736, 35795560, 9040490, 25342571, 12210793, 27898480, 22352499, 22631033, 41300601, 3478140, 24932989, 66982525, 184959, 35107462, 8073864, 8983183, 184979, 53916, 9933471, 529056, 2568868, 13824676, 2126501, 36450985, 13062829, 42332846, 70322, 46576311, 36598455, 29979321, 2036409, 6353598, 4800, 26866372, 62149, 62153, 54145741, 22901459, 56758995, 43610835, 11760348, 37032671, 13024, 67924707, 11891433, 2413296, 1897206, 33567480, 18805500, 692988, 67891964, 52146943, 14062336, 23032578, 54019, 25957127, 2110221, 3240723, 6984468, 37674, 201525, 50991931, 8598331, 18600765, 4920126, 439102, 41956157, 3945278, 2208584, 10818378, 26473291, 39727947, 14201682, 10179411, 881491, 381782, 406363, 19952476, 13149, 6050655, 11309920, 27833186, 2626421, 3609466, 61690747, 53343108, 430980, 37774, 897934, 47862672, 22549390, 70547, 29589, 16749463, 766875, 36484005, 742316, 5690287, 55825336, 1889217, 46023, 31265743, 27218900, 54229, 1151958, 19133401, 33321947, 38253534, 45691897, 48870396, 48534532, 32863238, 4338696, 58520592, 38007831, 25351191, 1184796, 529439, 58946599, 5608488, 594987, 2495537, 10957878, 50394170, 41047099, 734272, 57275457, 9917505, 3626050, 57685061, 1012806, 65967176, 685130, 43627595, 23114830, 35034191, 6009939, 43611227, 5213, 1111137, 226402, 2258017, 34829413, 41677925, 1127534, 13612149, 390263, 9483388, 35927164, 2045055, 29831, 3396753, 19592340, 6837398, 3781784, 38041, 47035547, 21197980, 36484254, 27767967, 2905253, 61699239, 2675881, 35804330, 169131, 51328172, 193714, 95411, 496821, 2446518, 1864891, 33084611, 5323, 32158923, 595149, 283858, 1217747, 3781845, 382165, 3511512, 42980570, 59798759, 5355, 644332, 50083054, 13550, 32158961, 7419133, 22910214, 349448, 2077960, 7927053, 23139600, 16184595, 718110, 28456222, 54351136, 218406, 1807656, 43603241, 54573, 37131566, 701741, 11834672, 20632884, 15586616, 54587, 62936384, 22254915, 21828, 56833348, 4355398, 17519943, 1152328, 59872594, 9041235, 37967188, 1045845, 36984150, 28153175, 5322079, 27555169, 54632, 47764842, 9303405, 13679, 42669424, 2520434, 529780, 1381751, 234876, 3233153, 177541, 60421513, 46482, 50533783, 48936348, 9000355, 193957, 13235623, 611752, 234921, 11859370, 25130414, 31667631, 636344, 16938429, 46526, 31102405, 10671559, 5699016, 3053003, 35673556, 44258772, 3610075, 579040, 1979873, 37901793, 1660387, 12727781, 41580016, 8001012, 17634819, 38404, 177671, 57275912, 5961225, 1840653, 2831887, 13383187, 38424, 349722, 6575642, 42694174, 41055774, 24909346, 194085, 9270823, 62871079, 10147369, 22992426, 59405867, 47150650, 6215230, 51729995, 22092, 22093, 1275470, 2152015, 5101133, 17208913, 5035602, 988753, 87644, 14759518, 71265, 43562594, 63198823, 23533163, 27375211, 10827381, 3470969, 30332, 50230910, 308868, 382599, 480905, 54923, 38539, 1431181, 38958735, 63121, 382619, 36968102, 60446377, 431786, 46626475, 456363, 26334893, 644781, 1562284, 10647218, 57579192, 25228985, 243389, 40867519, 202437, 1185479, 169672, 60888779, 1226448, 440018, 612052, 50575063, 60085976, 1291991, 16348889, 8304348, 52647644, 39368416, 4093674, 7173874, 628467, 12629761, 2152196, 1808135, 30475, 1775374, 17069844, 56440599, 538393, 7837468, 60823333, 4749106, 1849140, 34354998, 2012983, 2078529, 31323977, 54859596, 54736719, 27162455, 4765530, 3782493, 898916, 915309, 79732, 2865013, 8009590, 1652601, 6013, 13612929, 325507, 24008586, 19347339, 366477, 39171985, 23852947, 10180504, 43399069, 37779362, 33757091, 890793, 34756533, 997303, 579516, 1210300, 9557957, 60774345, 849869, 55449550, 47339470, 6264793, 243674, 6112, 23943140, 1038309, 46208997, 5806056, 40867822, 56023027, 645111, 907259, 3127300, 43997189, 56440839, 858120, 301064, 2881547, 22546, 16488467, 7215125, 67229718, 16635934, 3577886, 33372192, 53123104, 79906, 63567907, 59594788, 2390053, 2381862, 67754025, 5027882, 5330985, 432174, 1185840, 14384, 24361010, 2258995, 923698, 21944373, 49281083, 24213566, 26212422, 1243208, 10958920, 637004, 2881613, 25311310, 39303262, 2766942, 39614564, 6920292, 964717, 596078, 497794, 26941571, 47806602, 7362700, 4954263, 25049240, 9066654, 2963622, 10328235, 702635, 4552880, 7362738, 383162, 16160954, 18741438, 42416319, 2668736, 48658625, 18823362, 61962436, 30916, 22725, 1923274, 64444618, 29604056, 1104090, 3602651, 71899, 16308445, 3651805, 2005216, 5486819, 67279077, 3463398, 34617580, 14573, 5339378, 55539, 3528947, 2947322, 15325435, 50280699, 31185151, 40909056, 1063168, 4987149, 24385806, 54647064, 1480987, 29448480, 39206, 13719853, 3881265, 22411575, 317752, 3332410, 3012923, 31034, 4979006, 31201599, 8100167, 20289869, 1849681, 4028754, 29129051, 38574433, 54155622, 1071462, 62380392, 19437928, 8468850, 342398, 11647367, 4553108, 28096917, 3660182, 4659608, 28236187, 498076, 52140446, 1104286, 1956255, 19585449, 37018026, 2095532, 64502192, 1300923, 2963902, 20208066, 113092, 12638661, 5052869, 3881415, 678353, 2103762, 1350109, 22067678, 3611109, 924141, 276975, 55791, 162296, 924170, 88595, 43825684, 43194901, 2775579, 6690, 743971, 1309220, 7043619, 6560301, 29538871, 24615479, 11827767, 36928057, 2636347, 14926, 53672527, 64083, 4201044, 64085, 25959000, 6745, 64088, 5913182, 1907296, 39000674, 8305253, 34298473, 5118574, 29342321, 7928434, 5266044, 14342785, 26999426, 5003908, 6788, 1890951, 16161419, 44309134, 38156944, 21437073, 63011479, 44972706, 989858, 25295524, 8469156, 48241318, 36666029, 23213, 5290678, 31416, 30882491, 15037, 1702593, 3513027, 3594951, 35052231, 1514191, 2570959, 1645263, 334546, 9042644, 48315099, 4676317, 744158, 41974496, 580320, 60898024, 60283633, 1833715, 54737654, 1497849, 37026554, 3013373, 23298, 31496, 4463369, 62798611, 41974555, 62372638, 23329, 56099, 54655786, 2054954, 2399020, 1440567, 12106552, 28326718, 31550, 416589, 514894, 66591573, 61651800, 6060889, 28062197, 228190, 10836835, 4840292, 2161511, 318313, 50977642, 21699434, 61422444, 301932, 4881262, 1080173, 3046256, 60463979, 4225907, 40704884, 31607, 13687674, 8133498, 37297021, 59480957, 72573, 236424, 228234, 8092560, 2333593, 3505049, 38345627, 24386465, 53066659, 580517, 15271, 56232, 33110953, 1661867, 31664, 7089, 23174077, 3111871, 56081344, 3275717, 48364486, 63609800, 66935753, 4250574, 47987665, 43867095, 621531, 64487, 736238, 416752, 40475635, 5028853, 60587000, 1719292, 21863425, 47840259, 64516, 5372935, 3357705, 4512778, 6577165, 2030607, 2423824, 1235986, 4660251, 46677020, 58481694, 28253219, 5479462, 5413930, 66255918, 57228339, 818234, 30874683, 18611260, 41958461, 4750398, 38755390, 261193, 1104969, 4627535, 48212, 43441237, 3644514, 3136619, 580715, 12082283, 42163310, 62438513, 47840375, 5250192, 59899027, 72852, 72855, 72857, 72858, 45079706, 22199452, 310429, 72861, 72866, 3587235, 20323494, 72875, 72878, 72883, 4275382, 72889, 801978, 228541, 72893, 59800766, 13065408, 36543677, 60587209, 1858763, 35757264, 47586513, 48340, 1793236, 2546904, 29449433, 1211612, 1981660, 59440349, 23776, 45112545, 60300514, 17284321, 195809, 50404581, 621809, 15015154, 818420, 5692662, 2047222, 269558, 36961531, 728321, 11894026, 2489613, 703758, 1817874, 261396, 458008, 65166616, 498971, 64027932, 384286, 6159650, 21257512, 1244458, 2432299, 31915311, 1187122, 23862, 43334967, 859458, 32652613, 42736966, 245067, 11689293, 548182, 884056, 4242777, 9952602, 7036255, 10296674, 1613163, 515436, 48561519, 22494577, 49380722, 22265206, 1129847, 466299, 2612612, 19283335, 57294217, 13958538, 654732, 294295, 4668823, 318873, 46357913, 35306910, 50462114, 8076709, 1670565, 25623974, 7273911, 2997690, 24001, 21364162, 7249348, 59407816, 47799755, 56499662, 17055183, 26598863, 581077, 46374359, 64437720, 25173473, 51895777, 5774818, 39902695, 27196905, 28827117, 18169326, 548336, 15858, 28245491, 7912947, 7671, 66018807, 11451897, 4193788, 2399742, 278018, 61742595, 1015304, 8166922, 3923466, 23166476, 63241742, 41451023, 67493391, 6118940, 64159270, 25050663, 745000, 187946, 25271852, 359983, 7729, 44424763, 2833980, 171581, 654916, 9526854, 12754503, 47775306, 261709, 2702930, 21462612, 44629591, 10133081, 48732, 360030, 843361, 671331, 3112548, 4169317, 35847782, 67092078, 47775348, 41008758, 46448252, 106121, 5136015, 5676692, 7872152, 44146329, 1752731, 6405787, 491172, 2252455, 6889129, 42393261, 515758, 52985524, 3997367, 68181697, 147143, 20913864, 589513, 26951370, 671431, 5316295, 14343887, 3235536, 1179348, 302808, 12410589, 7364325, 1531621, 37584615, 3768042, 909036, 73453, 286455, 3546873, 630522, 39345917, 57851653, 696077, 63995662, 2899729, 2711314, 7959, 15392541, 42155809, 21356332, 925493, 50716473, 52281147, 925506, 32587, 32591, 32593, 499537, 450389, 45064023, 35356507, 6102876, 42360671, 42262371, 1236844, 32623, 229236, 49954680, 12263290, 66273148, 507776, 376707, 52412294, 4390795, 32817039, 3178385, 31793047, 2596759, 7479199, 2621344, 32047012, 8150951, 31186864, 237489, 36749242, 16315, 27377596, 1343426, 3342275, 59580357, 50364362, 819149, 2555865, 442333, 1277918, 1572831, 4456418, 901091, 60383204, 3653602, 39280615, 23388142, 15007729, 15712244]\n",
        "#print(len(docsID))"
      ],
      "metadata": {
        "id": "WimsAJ5y1CQt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will create our inverted index, by reading the wiki dump file from gcp bucket.\n",
        "Our inverted index will be saved in our GCP account storage, and will be used for our search engine.\n",
        "\n",
        "We will create 3 instances of inverted indexes: one for the title, one for the body and one for the anchor text.\n",
        "\n",
        "We will use the code of assignment 3.\n",
        "\n",
        "### Setup - General imports:"
      ],
      "metadata": {
        "id": "ceKasUC6sVaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import builtins\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0trKqje7Tov",
        "outputId": "360c43cc-1d32-46f5-afae-deb5296138e5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing, importing, and initializing PySpark"
      ],
      "metadata": {
        "id": "HnfqMLANt0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt-get update -qq\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install -q graphframes\n",
        "!pip install -q google-cloud-storage==1.43.0\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n",
        "spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n",
        "!wget -N -P $spark_jars $graphframes_jar\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from graphframes import *\n",
        "\n",
        "# create a spark context and session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkCtB_hcscgs",
        "outputId": "cec7a01c-e2a2-45f8-e1fb-438424306a52",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 100 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "\u001b[K     |████████████████████████████████| 154 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 106 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 109 kB 44.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.1 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires google-api-core<2dev,>=1.21.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.3.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25h--2022-01-03 20:44:50--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 13.32.87.63, 13.32.87.119, 13.32.87.68, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|13.32.87.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247880 (242K) [binary/octet-stream]\n",
            "Saving to: ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’\n",
            "\n",
            "graphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-01-03 20:44:50 (3.88 MB/s) - ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy wiki data"
      ],
      "metadata": {
        "id": "PMfVbTg-uSzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate our user with the email connected to our GCP account\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "3IMNAsqpuMWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy one wikidumps files \n",
        "\n",
        "#TODO - change to the complete wiki dump data file ('big wiki') when trying to build the real index\n",
        "from google.cloud import storage\n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import auth\n",
        "from inverted_index_gcp import * #todo - change to inverted index gcp\n",
        "full_path = \"gs://wikidata_preprocessed/*\"\n",
        "parquetFile = spark.read.parquet(full_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UNxteVcxuY3G",
        "outputId": "16ab3687-8314-44b4-f566-fc454a5237e4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6e3f875d9174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minverted_index_gcp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;31m#todo - change to inverted index gcp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gs://wikidata_preprocessed/*\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mparquetFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    299\u001b[0m                        int96RebaseMode=int96RebaseMode)\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o43.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:596)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create stopwords RDD"
      ],
      "metadata": {
        "id": "hCjoUX_Huq_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)"
      ],
      "metadata": {
        "id": "dRybod2rup2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this point, we will strat to build 3 inverted indeces, one for the body, one for the title, and one for the anchor text"
      ],
      "metadata": {
        "id": "GEovmmxRtOOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **function for all of the 3 indeces:**"
      ],
      "metadata": {
        "id": "6hFT9iVeFsuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word_count**: Count the frequency of each word in the given text (tf),\n",
        "  and choose if we want to remove stopword by using removeStopword boolean argument.\n",
        "  "
      ],
      "metadata": {
        "id": "dzZYSFp1GOOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(text, id, removeStopword):\n",
        "  '''Parameters:\n",
        "  -----------\n",
        "    text: str\n",
        "      Text/title/anchor text of one document\n",
        "    id: int\n",
        "      Document id\n",
        "  Returns:\n",
        "  --------\n",
        "    List of tuples\n",
        "      A list of (token, (doc_id, tf)) pairs \n",
        "      for example: [('right', (1857, 2)), ('approval', (1857, 154)),, ...]\n",
        "  '''\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  if removeStopword:\n",
        "    tokens = [token for token in tokens if token not in all_stopwords]\n",
        "  token_counter = Counter(tokens)\n",
        "  tokens_wo_dup = []\n",
        "  for token in tokens:\n",
        "     if token not in tokens_wo_dup:\n",
        "       tokens_wo_dup.append(token)\n",
        "  return [(token, (id, token_counter[token])) for token in tokens_wo_dup]"
      ],
      "metadata": {
        "id": "MYQNEkcYuhMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DL**: doc_id: doc_len dict"
      ],
      "metadata": {
        "id": "AIofW20d4r68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_DL(text, id, removeStopword):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  if removeStopword:\n",
        "    tokens = [token for token in tokens if token not in all_stopwords]\n",
        "  return((id,len(tokens)))"
      ],
      "metadata": {
        "id": "-iiKsnNY5JEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reduce_word_counts: operates on the pairs returned by word_count, gets a list of values (unsorted posting list) and sorts it."
      ],
      "metadata": {
        "id": "IM6GTgO-xBqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_word_counts(unsorted_pl):\n",
        "  ''' Returns a sorted posting list by wiki_id.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    unsorted_pl: list of tuples\n",
        "      A list of (wiki_id, tf) tuples \n",
        "  Returns:\n",
        "  --------\n",
        "    list of tuples\n",
        "      A sorted posting list.\n",
        "  '''\n",
        "  return sorted(unsorted_pl,key=lambda x: x[0])"
      ],
      "metadata": {
        "id": "5SH_BYSGw8gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the df for each word"
      ],
      "metadata": {
        "id": "2gWQm9MuyN0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate_df takes a posting list RDD and returns an RDD where each element is a (token, df) pair\n",
        "\n",
        "def calculate_df(postings):\n",
        "  ''' Takes a posting list RDD and calculate the df for each token.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    postings: RDD\n",
        "      An RDD where each element is a (token, posting_list) pair.\n",
        "  Returns:\n",
        "  --------\n",
        "    RDD\n",
        "      An RDD where each element is a (token, df) pair.\n",
        "  '''\n",
        "  return postings.map(lambda x: (x[0], len(x[1])))"
      ],
      "metadata": {
        "id": "vwCYx0kdyWYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partitioning and writing the index\n",
        "\n",
        "partitions the posting list, writes out each bucket, and returns information about the location on disk of each posting list."
      ],
      "metadata": {
        "id": "WNq0MNfz8qRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def partition_postings_and_write(postings):\n",
        "  ''' Partitions the posting lists into buckets, writes out \n",
        "  all posting lists in a bucket to disk, and returns the posting locations for \n",
        "  each bucket. Writing to disk use the function `write_a_posting_list`, a \n",
        "  static method implemented in inverted_index_colab.py under the InvertedIndex \n",
        "  class. \n",
        "  Parameters:\n",
        "  -----------\n",
        "    postings: RDD\n",
        "      An RDD where each item is a (w, posting_list) pair.\n",
        "  Returns:\n",
        "  --------\n",
        "    RDD\n",
        "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
        "      posting locations maintain a list for each word of file locations and \n",
        "      offsets its posting list was written to. See `write_a_posting_list` for \n",
        "      more details.\n",
        "  '''\n",
        "  r = postings.map(lambda x: (token2bucket_id(x[0]), x))\n",
        "  r1 = r.groupByKey().mapValues(list)\n",
        "  return r1.map(lambda x: InvertedIndex.write_a_posting_list(x))"
      ],
      "metadata": {
        "id": "0tpVrp-T83qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - only for debug, at the end- delete this cell\n",
        "TUPLE_SIZE = 6       \n",
        "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
        "from contextlib import closing\n",
        "\n",
        "def read_posting_list(inverted, w):\n",
        "  with closing(MultiFileReader()) as reader:\n",
        "    locs = inverted.posting_locs[w]\n",
        "    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n",
        "    posting_list = []\n",
        "    for i in range(inverted.df[w]):\n",
        "      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
        "      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
        "      posting_list.append((doc_id, tf))\n",
        "    return posting_list"
      ],
      "metadata": {
        "id": "mzKcbhS9B4S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - delete this section before submission - we will use the whole wiki corpus\n",
        "\n",
        "#add another RDD with the files of our train data set of queries (30), to make sure that we have those files in our small corpus\n",
        "df = parquetFile[parquetFile[\"id\"].isin(docsID)]"
      ],
      "metadata": {
        "id": "Bgspuh4IR6XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZitqpZBgL2VL",
        "outputId": "da4b47ba-6eb3-4220-8b6d-586590ee0cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1857, title='Approval voting', text='thumb|right|On an approval ballot, the voter can select any number of candidates.\\n\\'\\'\\'Approval voting\\'\\'\\' is an electoral system where each voter may select (\"approve\") any number of candidates, and the winner is the candidate approved by the largest number of voters. It is distinct from plurality voting, in which a voter may choose only one option among several, whereby the option with the most votes is chosen. It is related to score voting in which voters give each option a score on a scale, and the option with the highest total of scores is selected. Approval voting can also be used in multiwinner elections; see multiwinner approval voting.\\n\\nProposals to implement approval voting for municipal elections in the United States, were approved in referendums in Fargo, North Dakota, in 2018, and St. Louis, Missouri, in 2020. Fargo used approval voting in June 2020 to elect two at-large seats on its city council, and St. Louis used it to advance two candidates in March 2021 nonpartian primaries for mayor and aldermen.\\n\\n Description \\n\\nApproval voting ballots show a list of the candidates running for that seat for each office being contested. Next to each name is a checkbox (or another similar way to mark \"Yes\" or \"No\" for that candidate).\\n\\nEach candidate may be treated as a separate question: \"Do you approve of this person for the job?\" Approval voting lets each voter indicate support for one, some, or all candidates. All votes count equally, and everyone gets the same number of votes: one vote per candidate, either for or against. Final tallies show how many voters support each candidate, and the winner is the candidate whom the most voters support.\\n\\nBallots on which the voter marked every candidate the same (whether yes or no) usually have no effect on the outcome of the election. Each ballot separates candidates into two groups: those supported and those that are not. Each candidate approved is considered preferred to any candidate not approved, while the voter\\'s preferences among approved candidates is unspecified, and likewise, the voter\\'s preferences among unapproved candidates is also unspecified.\\n Usage \\n Current use \\nIn 2018, Fargo, North Dakota, passed a local ballot initiative adopting approval voting for the city\\'s local elections, and it was used to elect officials in June 2020, becoming the first United States city and jurisdiction to adopt approval voting.\\n\\nIn November 2020, St. Louis, Missouri passed Proposition D to authorize a variant of approval voting (as unified primary) for municipal offices.\\n\\nHistory  \\n300px|thumb|Rows of secret approval vote boxes from early 1900s Greece, where the voter drops a marble to the right or left of the box, through a tube, one for each candidate standing\\nRobert J. Weber coined the term \"approval voting\" in 1971. It was more fully published in 1978 by political scientist Steven Brams and mathematician Peter Fishburn.\\n\\nHistorically, several voting methods that incorporate aspects of approval voting have been used:\\n\\n Approval voting was used for papal conclaves between 1294 and 1621, with an average of about forty cardinals engaging in repeated rounds of voting until one candidate was listed on at least two-thirds of ballots.  Josep Colomer writes of the 1559 conclave when a cardinal nearly won when an ally met confidentially with cardinals and asked them for a \"token\" approval vote for his friend to avoid a shut out – and then the cardinal came close to winning with votes on 17 of 32 ballots.\\n In the 13th through 18th centuries, the Republic of Venice elected the Doge of Venice using a multi-stage process that featured random selection and voting that allowed approval of multiple candidates and required a supermajority.\\n According to Steven J. Brams, approval voting was used for unspecified elections in 19th century England.\\n The selection of the Secretary-General of the United Nations has involved \"straw poll\" rounds of approval polling to help discover and build a consensus before a formal vote is held in the Security Council. The United Nations Secretary-General selection, 2006 indicated that South Korean Foreign Minister Ban Ki-moon was the only candidate to be acceptable to all five permanent members of the Security Council, which led to the withdrawal of India\\'s Shashi Tharoor, who had the highest overall approval rate.\\nApproval voting was used in Greek legislative elections from 1864 to 1923, when it was replaced with proportional representation.\\n\\n Political organizations and jurisdictions \\nApproval voting has been used in privately administered nomination contests by the Independent Party of Oregon in 2011, 2012, 2014, and 2016. Oregon is a fusion voting state, and the party has cross-nominated legislators and statewide officeholders using this method; its 2016 presidential preference primary did not identify a potential nominee due to no candidate earning more than 32% support. The party switched to using STAR voting in 2020.\\n\\nIt is also used in internal elections by the American Solidarity Party, the Green Parties of Texas and Ohio, the Libertarian parties of Texas and Colorado, the US Modern Whig party, and the German Pirate Party.\\n\\nIn 2018, Fargo, North Dakota passed a ballot initiative adopting approval voting for local elections, becoming the first US city and jurisdiction to adopt approval voting.  (A previous city commissioner election in 2015 suffered from six-way vote-splitting, resulting in a candidate winning with only a 22% plurality of the vote.) The first election was held June 9, 2020, selecting two city commissioners.  Both winners received over 50% approval, with an average 2.3 approvals per ballot, and 62% of voters supported the change to approval voting in a poll. A poll by opponents of approval voting was conducted to test whether voters had in fact voted strategically according to the Burr dilemma. They found that 30% of voters who bullet voted did so for strategic reasons, while 57% did so because it was their sincere opinion. \\n\\nIn 2020, St. Louis, Missouri passed an initiative to adopt approval voting followed by a top-two runoff (see Unified primary), thus becoming the second US city to adopt approval voting and the first to use a variant of it.  The first such primary was held in March 2021, and unofficial results showed voters expressed 1.1 to 1.6 approvals per ballot, in races with more than two candidates.\\n\\n Other organizations \\nThe idea of approval was adopted by X. Hu and Lloyd Shapley in 2003 in studying authority distribution in organizations.\\n\\nApproval voting has been adopted by several learned societies: the Society for Social Choice and Welfare (1992), Mathematical Association of America (1986), the American Mathematical Society, the Institute of Management Sciences (1987) (now the Institute for Operations Research and the Management Sciences), the American Statistical Association (1987), and the Institute of Electrical and Electronics Engineers (1987). The IEEE board in 2002 rescinded its decision to use approval voting. IEEE Executive Director Daniel J. Senese stated that approval voting was abandoned because \"few of our members were using it and it was felt that it was no longer needed.\" Because none of these associations report results to their members and the public, it is difficult to evaluate Senese\\'s claim and whether it is also true of other associations; Steven Brams\\' analysis of the 5-candidate 1987 Mathematical Association of America presidential election shows that 79% of voters cast a ballot for one candidate, 16% for 2 candidates, 5% for 3, and 1% for 4, with the winner earning the approval of 1,267 (32%) of 3,924 voters.\\n\\nApproval voting was used for Dartmouth Alumni Association elections for seats on the College Board of Trustees, but after some controversy it was replaced with traditional runoff elections by an alumni vote of 82% to 18% in 2009. Dartmouth students started to use approval voting to elect their student body president in 2011. In the first election, the winner secured the support of 41% of voters against several write-in candidates. In 2012, Suril Kantaria won with the support of 32% of the voters. In 2013, 2014 and 2016, the winners also earned the support of under 40% of the voters. Results reported in \\'\\'The Dartmouth\\'\\' show that in the 2014 and 2016 elections, more than 80 percent of voters approved of only one candidate. Students replaced approval voting with plurality voting before the 2017 elections.\\n\\nApproval voting also can be used in social scenarios as a fairer, but still quick system compared to a First-Past-The-Post equivalent, being able to avoid a spoiler effect while being very quick to calculate.\\n\\nSee also: Multiwinner approval voting#Usage.\\n\\n Effect on elections \\n\\nApproval voting advocates Steven Brams and Dudley R. Herschbach predict that approval voting should increase voter participation, prevent minor-party candidates from being spoilers, and reduce negative campaigning. The effect of this system as an electoral reform measure is not without critics, however. FairVote has a position paper arguing that approval voting has three flaws that undercut it as a method of voting and political vehicle. They argue that it can result in the defeat of a candidate who would win an absolute majority in a plurality election, can allow a candidate to win who might not win \\'\\'any\\'\\' support in a plurality election, and has incentives for tactical voting.  The first two \"flaws\" are considered advantages by advocates of approval voting, as it chooses centrist candidates with broad appeal rather than polarizing candidates who appeal only to the majority. Supporters also point out that any voting method is subject to tactical voting with more than two candidates, as pointed out in Gibbard\\'s theorem.\\n\\nOne study showed that approval voting would not have chosen the same two winners as plurality voting (Chirac and Le Pen) in France\\'s presidential election of 2002 (first round) – it instead would have chosen Chirac and Jospin as the top two to proceed to a runoff. Le Pen lost by a very high margin in the runoff, 82.2% to 17.8%, a sign that the true top two had not been found. Straight approval voting without a runoff, from the study, still would have selected Chirac, but with an approval percentage of only 36.7%, compared to Jospin at 32.9%. Le Pen, in that study, would have received 25.1%. In the real primary election, the top three were Chirac, 19.9%, Le Pen, 16.9%, and Jospin, 16.2%. A study of various \"evaluative voting\" methods (approval voting and score voting) during the French presidential election, 2012 showed that \"unifying\" candidates tended to do better, and polarizing candidates did worse, via the evaluative voting methods than via the plurality system.\\n\\nA generalized version of the Burr dilemma applies to approval voting when two candidates are appealing to the same subset of voters.  Although approval voting differs from the voting system used in the Burr dilemma, approval voting can still leave candidates and voters with the generalized dilemma of whether to compete or cooperate.\\n\\nWhile in the modern era there have been relatively few competitive approval voting elections where tactical voting is more likely, Brams argues that approval voting usually elects Condorcet winners in practice. Critics of the use of approval voting in the alumni elections for the Dartmouth Board of Trustees in 2009 placed its ultimately successful repeal before alumni voters, arguing that the system has not been electing the most centrist candidates. \\'\\'The Dartmouth\\'\\' editorialized that \"When the alumni electorate fails to take advantage of the approval voting process, the three required Alumni Council candidates tend to split the majority vote, giving petition candidates an advantage. By reducing the number of Alumni Council candidates, and instituting a more traditional one-person, one-vote system, trustee elections will become more democratic and will more accurately reflect the desires of our alumni base.\"\\n\\n Strategic voting \\n\\n Overview \\nApproval voting allows voters to select all the candidates whom they consider to be reasonable choices.\\n\\n\\'\\'Strategic approval\\'\\' voting differs from ranked choice voting methods where voters might \\'\\'reverse\\'\\' the preference order of two options, which if done on a larger scale causes an unpopular candidate to win. Strategic Approval voting, with more than two options, involves the voter changing their approval threshold. The voter decides which options to give the \\'\\'same\\'\\' rating, even if they were to have a preference order between them.\\n\\nApproval voting allows for bullet voting and compromising,  while it is immune to push-over and burying.\\n\\nBullet Voting occurs when a voter approves \\'\\'only\\'\\' candidate \\'a\\' instead of \\'\\'both\\'\\' \\'a\\' and \\'b\\' for the reason that voting for \\'b\\' can cause \\'a\\' to lose. The voter would be satisfied with either \\'a\\' or \\'b\\' but has a moderate preference for \\'a\\'. Were \\'b\\' to win, this hypothetical voter would still be satisfied.\\n\\nCompromising occurs when a voter approves an \\'\\'additional\\'\\' candidate who is otherwise considered unacceptable to the voter to prevent an even worse alternative from winning.\\n\\n Sincere voting \\nApproval voting experts describe sincere votes as those \"... that directly reflect the true preferences of a voter, i.e., that do not report preferences \\'falsely.\\'\"  They also give a specific definition of a sincere approval vote in terms of the voter\\'s ordinal preferences as being any vote that, if it votes for one candidate, it also votes for any more preferred candidate.  This definition allows a sincere vote to treat strictly preferred candidates the same, ensuring that every voter has at least one sincere vote. The definition also allows a sincere vote to treat equally preferred candidates differently.  When there are two or more candidates, every voter has at least three sincere approval votes to choose from.  Two of those sincere approval votes do not distinguish between any of the candidates: vote for none of the candidates and vote for all of the candidates.  When there are three or more candidates, every voter has more than one sincere approval vote that distinguishes between the candidates.\\n\\n Examples \\n\\nBased on the definition above, if there are four candidates, A, B, C, and D, and a voter has a strict preference order, preferring A to B to C to D, then the following are the voter\\'s possible sincere approval votes:\\nvote for A, B, C, and D\\nvote for A, B, and C\\nvote for A and B\\nvote for A\\nvote for no candidates\\n\\nIf the voter instead equally prefers B and C, while A is still the most preferred candidate and D is the least preferred candidate, then all of the above votes are sincere and the following combination is also a sincere vote:\\nvote for A and C\\n\\nThe decision between the above ballots is equivalent to deciding an arbitrary \"approval cutoff.\" All candidates preferred to the cutoff are approved, all candidates less preferred are not approved, and any candidates equal to the cutoff may be approved or not arbitrarily.\\n\\n Sincere strategy with ordinal preferences \\n\\nA sincere voter with multiple options for voting sincerely still has to choose which sincere vote to use. Voting strategy is a way to make that choice, in which case strategic approval voting includes sincere voting, rather than being an alternative to it. This differs from other voting systems that typically have a unique sincere vote for a voter.\\n\\nWhen there are three or more candidates, the winner of an approval voting election can change, depending on which sincere votes are used. In some cases, approval voting can sincerely elect any one of the candidates, including a Condorcet winner and a Condorcet loser, without the voter preferences changing.  To the extent that electing a Condorcet winner and not electing a Condorcet loser is considered desirable outcomes for a voting system, approval voting can be considered vulnerable to sincere, strategic voting.  In one sense, conditions where this can happen are robust and are not isolated cases.  On the other hand, the variety of possible outcomes has also been portrayed as a virtue of approval voting, representing the flexibility and responsiveness of approval voting, not just to voter ordinal preferences, but cardinal utilities as well.\\n\\n Dichotomous preferences \\n\\nApproval voting avoids the issue of multiple sincere votes in special cases when voters have dichotomous preferences. For a voter with dichotomous preferences, approval voting is strategy-proof (also known as strategy-free).  When all voters have dichotomous preferences and vote the sincere, strategy-proof vote, approval voting is guaranteed to elect the Condorcet winner, if one exists.  However, having dichotomous preferences when there are three or more candidates is not typical.  It is an unlikely situation for all voters to have dichotomous preferences when there are more than a few voters.\\n\\nHaving dichotomous preferences means that a voter has bi-level preferences for the candidates.  All of the candidates are divided into two groups such that the voter is indifferent between any two candidates in the same group and any candidate in the top-level group is preferred to any candidate in the bottom-level group.  A voter that has strict preferences between three candidates—prefers A to B and B to C—does not have dichotomous preferences.\\n\\nBeing strategy-proof for a voter means that there is a unique way for the voter to vote that is a strategically best way to vote, regardless of how others vote. In approval voting, the strategy-proof vote, if it exists, is a sincere vote.\\n Approval threshold \\n\\nAnother way to deal with multiple sincere votes is to augment the ordinal preference model with an approval or acceptance threshold.  An approval threshold divides all of the candidates into two sets, those the voter approves of and those the voter does not approve of.  A voter can approve of more than one candidate and still prefer one approved candidate to another approved candidate.  Acceptance thresholds are similar.  With such a threshold, a voter simply votes for every candidate that meets or exceeds the threshold.\\n\\nWith threshold voting, it is still possible to not elect the Condorcet winner and instead elect the Condorcet loser when they both exist.  However, according to Steven Brams, this represents a strength rather than a weakness of approval voting.  Without providing specifics, he argues that the pragmatic judgements of voters about which candidates are acceptable should take precedence over the Condorcet criterion and other social choice criteria.\\n\\n Strategy with cardinal utilities \\n\\nVoting strategy under approval is guided by two competing features of approval voting.  On the one hand, approval voting fails the later-no-harm criterion, so voting for a candidate can cause that candidate to win instead of a candidate more preferred by that voter.  On the other hand, approval voting satisfies the monotonicity criterion, so not voting for a candidate can never help that candidate win, but can cause that candidate to lose to a less preferred candidate.  Either way, the voter can risk getting a less preferred election winner.  A voter can balance the risk-benefit trade-offs by considering the voter\\'s cardinal utilities, particularly via the von Neumann–Morgenstern utility theorem, and the probabilities of how others vote.\\n\\nA rational voter model described by Myerson and Weber specifies an approval voting strategy that votes for those candidates that have a positive prospective rating.  This strategy is optimal in the sense that it maximizes the voter\\'s expected utility, subject to the constraints of the model and provided the number of other voters is sufficiently large.\\n\\nAn optimal approval vote always votes for the most preferred candidate and not for the least preferred candidate.  However, an optimal vote can require voting for a candidate and not voting for a more preferred candidate if there 4 candidates or more.\\n\\nOther strategies are also available and coincide with the optimal strategy in special situations.  For example:\\n Vote for the candidates that have above average utility.  This strategy coincides with the optimal strategy if the voter thinks that all pairwise ties are equally likely\\n Vote for any candidate that is more preferred than the expected winner and also vote for the expected winner if the expected winner is more preferred than the expected runner-up.  This strategy coincides with the optimal strategy if there are three or fewer candidates or if the pivot probability for a tie between the expected winner and expected runner-up is sufficiently large compared to the other pivot probabilities.\\nVote for the most preferred candidate only.  This strategy coincides with the optimal strategy when there is only one candidate with a positive prospective rating.\\n\\nAnother strategy is to vote for the top half of the candidates, the candidates that have an above-median utility. When the voter thinks that others are balancing their votes randomly and evenly, the strategy maximizes the voter\\'s power or efficacy, meaning that it maximizes the probability that the voter will make a difference in deciding which candidate wins.\\n\\nOptimal strategic approval voting fails to satisfy the Condorcet criterion and can elect a Condorcet loser.  Strategic approval voting can guarantee electing the Condorcet winner in some special circumstances.  For example, if all voters are rational and cast a strategically optimal vote based on a common knowledge of how all the other voters vote except for small-probability, statistically independent errors in recording the votes, then the winner will be the Condorcet winner, if one exists.\\n\\n Strategy examples \\nIn the example election described here, assume that the voters in each faction share the following von Neumann–Morgenstern utilities, fitted to the interval between 0 and 100.  The utilities are consistent with the rankings given earlier and reflect a strong preference each faction has for choosing its city, compared to weaker preferences for other factors such as the distance to the other cities.\\n\\n+Voter utilities for each candidate city \\xa0  Candidates  \\xa0 Fraction of voters(living close to) Memphis Nashville Chattanooga Knoxville Average Memphis (42%) 100  15  10  0  31.25 Nashville (26%) 0  100  20  15  33.75 Chattanooga (15%) 0  15  100  35  37.5 Knoxville (17%) 0  15  40  100  38.75\\n\\nUsing these utilities, voters choose their optimal strategic votes based on what they think the various pivot probabilities are for pairwise ties.  In each of the scenarios summarized below, all voters share a common set of pivot probabilities.\\n\\n+Approval voting results  for scenarios using optimal strategic voting \\xa0  Candidate vote totals Strategy scenario Winner Runner-up Memphis Nashville Chattanooga Knoxville Zero-info Memphis  Chattanooga  42  26  32  17 Memphis leading Chattanooga Three-way tie  42  58  58  58 Chattanooga leading Knoxville Chattanooga  Nashville  42  68  83  17 Chattanooga leading Nashville Nashville  Memphis  42  68  32  17 Nashville leading Memphis Nashville  Memphis  42  58  32  32\\n\\nIn the first scenario, voters all choose their votes based on the assumption that all pairwise ties are equally likely.  As a result, they vote for any candidate with an above-average utility.  Most voters vote for only their first choice.  Only the Knoxville faction also votes for its second choice, Chattanooga.  As a result, the winner is Memphis, the Condorcet loser, with Chattanooga coming in second place.  In this scenario, the winner has minority approval (more voters disapproved than approved) and all the others had even less support, reflecting the position that no choice gave an above-average utility to a majority of voters.\\n\\nIn the second scenario, all of the voters expect that Memphis is the likely winner, that Chattanooga is the likely runner-up, and that the pivot probability for a Memphis-Chattanooga tie is much larger than the pivot probabilities of any other pair-wise ties.  As a result, each voter votes for any candidate they prefer more than the leading candidate, and also vote for the leading candidate if they prefer that candidate more than the expected runner-up. Each remaining scenario follows a similar pattern of expectations and voting strategies.\\n\\nIn the second scenario, there is a three-way tie for first place.  This happens because the expected winner, Memphis, was the Condorcet loser and was also ranked last by any voter that did not rank it first.\\n\\nOnly in the last scenario does the actual winner and runner-up match the expected winner and runner-up.  As a result, this can be considered a stable strategic voting scenario. In the language of game theory, this is an \"equilibrium.\"  In this scenario, the winner is also the Condorcet winner.\\n\\nDichotomous cutoff\\nAs this voting method is cardinal rather than ordinal, it is possible to model voters in a way that does not simplify to an ordinal method. Modelling voters with a \\'dichotomous cutoff\\' assumes a voter has an immovable approval cutoff, while having meaningful cardinal preferences. This means that rather than voting for their top 3 candidates, or all candidates above the average approval (which may result in their vote changing if one candidate drops out, resulting in a system that does not satisfy IIA), they instead vote for all candidates above a certain approval \\'cutoff\\' that they have decided. This cutoff does not change, regardless of which and how many candidates are running, so when all available alternatives are either above or below the cutoff, the voter votes for all or none of the candidates, despite preferring some over others.  This could be imagined to reflect a case where many voters become disenfranchised and apathetic if they see no candidates they approve of. In a case such as this, many voters may have an internal cutoff, and would not simply vote for their top 3, or the above average candidates, although that is not to say that it is necessarily entirely immovable.\\n\\nFor example, in this scenario, voters are voting for candidates with approval above 50% (bold signifies that the voters voted for the candidate):\\n\\n Proportion of electorate Approval of Candidate A Approval of Candidate B Approval of Candidate C Approval of Candidate D Average approval 25% \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  40%  10%  \\'\\'50%\\'\\' 35% 10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  40%  \\'\\'50%\\'\\' 30% 40%  10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  \\'\\'50%\\'\\' 10% \\'\\'\\'60%\\'\\'\\'  40%  10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'50%\\'\\'\\n\\nC wins with 65% of the voters\\' approval, beating B with 60%, D with 40% and A with 35%\\n\\nIf voters\\' threshold for receiving a vote is that the candidate has an above average approval, or they vote for their two most approved of candidates, this is not a dichotomous cutoff, as this can change if candidates drop out.  On the other hand, if voters\\' threshold for receiving a vote is fixed (say 50%), this is a dichotomous cutoff, and satisfies IIA as shown below:\\n\\n+ A drops out, candidates voting for above average approval Proportion of electorate Approval of Candidate A Approval of Candidate B Approval of Candidate C Approval of Candidate D Average approval 25% –  \\'\\'\\'60%\\'\\'\\'  \\'\\'\\'40%\\'\\'\\'  10%  \\'\\'37%\\'\\' 35% –  \\'\\'\\'90%\\'\\'\\'  60%  40%  \\'\\'63%\\'\\' 30% –  10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  \\'\\'53%\\'\\' 10% –  40%  10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'47%\\'\\'\\nB now wins with 60%, beating C with 55% and D with 40%\\n+ A drops out, candidates voting for approval > 50% Proportion of electorate Approval of Candidate A Approval of Candidate B Approval of Candidate C Approval of Candidate D Average approval 25% –  \\'\\'\\'60%\\'\\'\\'  40%  10%  \\'\\'37%\\'\\' 35% –  \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  40%  \\'\\'63%\\'\\' 30% –  10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  \\'\\'53%\\'\\' 10% –  40%  10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'47%\\'\\'\\nWith dichotomous cutoff, C still wins.\\n\\n+ D drops out, candidates voting for top 2 candidates Proportion of electorate Approval of Candidate A Approval of Candidate B Approval of Candidate C Approval of Candidate D Average approval 25% \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  40%  –  \\'\\'63%\\'\\' 35% 10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  –  \\'\\'53%\\'\\' 30% \\'\\'\\'40%\\'\\'\\'  10%  \\'\\'\\'90%\\'\\'\\'  –  \\'\\'47%\\'\\' 10% \\'\\'\\'60%\\'\\'\\'  \\'\\'\\'40%\\'\\'\\'  10%  –  \\'\\'37%\\'\\'\\nB now wins with 70%, beating C and A with 65%\\n+ D drops out, candidates voting for approval > 50% Proportion of electorate Approval of Candidate A Approval of Candidate B Approval of Candidate C Approval of Candidate D Average approval 25% \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  40%  –  \\'\\'63%\\'\\' 35% 10%  \\'\\'\\'90%\\'\\'\\'  \\'\\'\\'60%\\'\\'\\'  –  \\'\\'53%\\'\\' 30% 40%  10%  \\'\\'\\'90%\\'\\'\\'  –  \\'\\'47%\\'\\' 10% \\'\\'\\'60%\\'\\'\\'  40%  10%  –  \\'\\'37%\\'\\'\\nWith dichotomous cutoff, C still wins.\\n\\n Compliance with voting system criteria \\nMost of the mathematical criteria by which voting systems are compared were formulated for voters with ordinal preferences. In this case, approval voting requires voters to make an additional decision of where to put their approval cutoff (see examples above). Depending on how this decision is made, approval voting satisfies different sets of criteria.\\n\\nThere is no ultimate authority on which criteria should be considered, but the following are criteria that many voting theorists accept and consider desirable:\\n Unrestricted domain—A voter may have any preference ordering among the alternatives.\\n Non-dictatorship—There does not exist a single voter whose preference for the alternatives always determines the outcome regardless of other voters\\' preferences.\\n Pareto efficiency—If every voter prefers candidate A to all other candidates, then A must be elected. (from Arrow\\'s impossibility theorem)\\n Majority criterion—If there exists a majority that ranks (or rates) a single candidate higher than all other candidates, does that candidate always win?\\n Monotonicity criterion—Is it impossible to cause a winning candidate to lose by ranking that candidate higher, or to cause a losing candidate to win by ranking that candidate lower?\\n Consistency criterion—If the electorate is divided in two and a choice wins in both parts, does it always win overall?\\n Participation criterion—Is voting honestly always better than not voting at all? (This is grouped with the distinct but similar Consistency Criterion in the table below.)\\n Condorcet criterion—If a candidate beats every other candidate in pairwise comparison, does that candidate always win? (This implies the majority criterion, above)\\n Condorcet loser criterion—If a candidate loses to every other candidate in pairwise comparison, does that candidate always lose?\\n Independence of irrelevant alternatives—Is the outcome the same after adding or removing non-winning candidates?\\n Independence of clones criterion—Is the outcome the same if candidates identical to existing candidates are added?\\n Reversal symmetry—If individual preferences of each voter are inverted, does the original winner never win?\\n\\n  Unrestricted domain  Non-dictatorship  Pareto efficiency  Majority  Monotone  Consistency & Participation  Condorcet  Condorcet loser  IIA  Clone independence  Reversal symmetry Cardinal preferences  Zero information, rational voters Yes Yes No No Yes Yes No No No  No Yes Imperfect information, rational voters Yes Yes No No Yes Yes No No No No Yes Strong Nash equilibrium (Perfect information, rational voters, and perfect strategy) Yes Yes Yes Yes  Yes  No Yes No No  Yes  Yes Absolute dichotomous cutoff Yes No Yes No Yes Yes No No Yes Yes Yes Dichotomous preferences  Rational voters No Yes Yes Yes  Yes  Yes  Yes  Yes  Yes  Yes  Yes\\n\\nApproval voting satisfies the mutual majority criterion and Smith criterion when voters\\' preferences are dichotomous; this is because the winner will be someone that the most voters prefer above all others, or that ties with other candidates but the group of tied candidates is preferred by more voters than any candidate not in the group.\\n\\n Other issues and comparisons \\n\\n Approval voting can allow voters to cast a compromise vote without abandoning their favorite candidate as long as voters accept the potential of that compromise vote resulting in the defeat of their favorite. Plurality voting can lead to voters abandoning their first choice in order to help a \"lesser of evils\" to win.\\n However, approval voting forces voters to face an initial voting tactical decision as to whether to vote for (or \\'\\'approve of\\'\\') their second-choice candidate or not. The voter may want to retain expression of preference of their favorite candidate over their second choice.  But that does not allow the same voter to express preference of their second choice over any other.\\n Approval ballots can be counted by existing machines designed for plurality elections, as ballots are cast, so that final tallies are immediately available after the election, without any upgrades to equipment. Approval counting can be completed at the local level and conveniently summed at the regional or national level.\\n If voters are sincere, approval voting would elect centrists at least as often as moderates of each extreme. If backers of relatively extreme candidates are insincere and \"bullet vote\" for that first choice, they can help that candidate defeat a compromise candidate who would have won if every voter had cast sincere preferences.\\n If voters are sincere, candidates trying to win an approval voting election might need as much as 100% approval to beat a strong competitor, and would have to find solutions that are fair to everyone to do so. However, a candidate may win a plurality race by promising many perks to a simple majority or even a plurality of voters at the expense of the smaller voting groups.\\n Approval voting fails the majority criterion, because a candidate who is preferred by a majority of voters is not always elected. In some cases approval voting will elect a candidate that has greater overall utility than a candidate preferred by a mere majority, if the majority also approves a compromise candidate that includes representation of the minority. In other cases, with elections having three or more candidates, approval voting will fail to elect the candidate with greater overall utility also preferred by a majority, if a less moderate candidate within the majority view gains enough approvals from the majority to win, while core supporters of the less moderate candidate are more selective (i.e. vote only for the extreme candidate), leaving a third sizable minority unrepresented.  It can also fail a different majority criterion in that the winner can win with fewer than half the votes approving. \\n Suppose a candidate is eliminated (say, for medical reasons) between a primary election and the party convention. With plurality voting, voters who supported the eliminated candidate lose their franchise. Approval voting affords representation to voters by counting their approvals among remaining candidates.\\n Approval voting without write-ins is easily reversed as disapproval voting where a choice is disavowed, as is already required in other measures in politics (e.g., representative recall).\\n Unlike plurality voting, approval voting allows voters to block a candidate by voting for several alternatives instead of just one, increasing the probability an alternative wins.\\n In contentious elections with large groups of organized voters who prefer their favorite candidate vastly over all others, approval voting may revert to plurality voting. Some voters support only their single favored candidate when they perceive the other candidates more as competitors to their preferred candidate than as compromise choices. Score voting and Majority Judgment allow these voters to give intermediate approval ratings, but at the cost of added ballot complexity and longer ballot counts.\\n\\n Ballot types \\nApproval ballots can be of at least four semi-distinct forms. The simplest form is a blank ballot on which voters hand-write the names of the candidates they support. A more structured ballot lists all candidates, and voters mark each candidate they support. A more explicit structured ballot can list the candidates and provide two choices by each. (Candidate list ballots can include spaces for write-in candidates as well.)\\n 160px 160px 160px 160px\\n\\nAll four ballots are theoretically equivalent. The more structured ballots may aid voters in offering clear votes so they explicitly know all their choices. The Yes/No format can help to detect an \"undervote\" when a candidate is left unmarked and allow the voter a second chance to confirm the ballot markings are correct. The \"single bubble\" format is incapable of producing invalid ballots (which might otherwise be rejected in counting).\\n\\nUnless the second or fourth format is used, fraudulently adding votes to an approval voting ballot does not invalidate the ballot (that is, it does not make it appear inconsistent). Thus, approval voting raises the importance of ensuring that the \"chain of custody\" of ballots is secure.\\n\\n See also \\n\\n Multiwinner approval voting - a variant of approval voting in which multiple candidates may be elected.\\nParty-approval voting - a variant of approval voting in which voters approve of parties, rather than individual candidates.\\n Score Voting (also called Range voting) - a generalization of approval voting in which each voter can give any score to any voter, rather than just 0 or 1.\\n\\n Notes \\n\\n References \\n\\n Sources \\n \\n\\n External links \\n\\n Approval Voting Article by The Center for Election Science\\n Could Approval Voting Prevent Electoral Disaster? Video by Big Think\\n Approval Voting on Dichotomous Preferences Article by Marc Vorsatz.\\n Scoring Rules on Dichotomous Preferences Article by Marc Vorsatz.\\n The Arithmetic of Voting article by Guy Ottewell\\n Critical Strategies Under Approval Voting: Who Gets Ruled In And Ruled Out Article by Steven J. Brams and M. Remzi Sanver.\\n Quick and Easy Voting for Normal People YouTube video\\n\\nCategory:Single-winner electoral systems\\nCategory:Cardinal electoral systems\\nCategory:Monotonic electoral systems\\nCategory:Electoral systems\\nCategory:Historical rankings of public figures\\nCategory:Rating', anchor_text=[Row(id=29066482, text='electoral system'), Row(id=10880, text='plurality voting'), Row(id=25908, text='score voting'), Row(id=68033962, text='multiwinner approval voting'), Row(id=22767486, text='Ballotpedia'), Row(id=65885421, text='nonpartian primaries for'), Row(id=128485, text='Fargo, North Dakota'), Row(id=27687, text='St. Louis, Missouri'), Row(id=479210, text='secret'), Row(id=12108, text='Greece'), Row(id=2222268, text='Robert J. Weber'), Row(id=174860, text='Steven Brams'), Row(id=1787186, text='Peter Fishburn'), Row(id=827717, text='papal conclave'), Row(id=62143071, text='Josep Colomer'), Row(id=613492, text='Republic of Venice'), Row(id=63340, text='Doge of Venice'), Row(id=162415, text='Secretary-General'), Row(id=31769, text='United Nations'), Row(id=14400375, text='United Nations Secretary-General selection, 2006'), Row(id=770162, text='Shashi Tharoor'), Row(id=12108, text='Greek'), Row(id=12008038, text='Independent Party of Oregon'), Row(id=46699, text='fusion voting'), Row(id=346055, text='The Oregonian'), Row(id=57871745, text='STAR voting'), Row(id=42806136, text='American Solidarity Party'), Row(id=403244, text='Green Parties'), Row(id=5226603, text='Texas'), Row(id=11945308, text='Ohio'), Row(id=32044, text='Libertarian parties'), Row(id=10083285, text='Texas'), Row(id=5130911, text='Colorado'), Row(id=18328648, text='Modern Whig'), Row(id=23159376, text='German Pirate Party'), Row(id=23159376, text='German Pirate Party'), Row(id=128485, text='Fargo, North Dakota'), Row(id=22767486, text='Ballotpedia'), Row(id=538393, text='vote-splitting'), Row(id=235892, text='plurality'), Row(id=2407835, text='bullet voted'), Row(id=27687, text='St. Louis, Missouri'), Row(id=25932, text='top-two runoff'), Row(id=41550632, text='Unified primary'), Row(id=22767486, text='Ballotpedia'), Row(id=345974, text='St. Louis Post-Dispatch'), Row(id=1135560, text='Lloyd Shapley'), Row(id=48729126, text='authority distribution'), Row(id=198772, text='Mathematical Association of America'), Row(id=198772, text='Mathematical Association of America'), Row(id=198822, text='American Mathematical Society'), Row(id=198822, text='American Mathematical Society'), Row(id=9283668, text='Institute for Operations Research and the Management Sciences'), Row(id=341988, text='American Statistical Association'), Row(id=341988, text='American Statistical Association'), Row(id=56938, text='Institute of Electrical and Electronics Engineers'), Row(id=1329211, text='American Political Science Association'), Row(id=261709, text='First-Past-The-Post'), Row(id=538393, text='spoiler effect'), Row(id=68033962, text='Multiwinner approval voting'), Row(id=174867, text='Dudley R. Herschbach'), Row(id=23475293, text='electoral reform'), Row(id=2088552, text='FairVote'), Row(id=1725354, text='who would win an absolute majority in a plurality election'), Row(id=30332, text='tactical voting'), Row(id=53957416, text=\"Gibbard's theorem\"), Row(id=39764, text='Chirac'), Row(id=49955, text='Le Pen'), Row(id=147060, text=\"France's presidential election of 2002\"), Row(id=22592845, text='the French presidential election, 2012'), Row(id=2407835, text='Burr dilemma'), Row(id=1133911, text='Condorcet winners'), Row(id=30332, text='bullet voting'), Row(id=30332, text='compromising'), Row(id=30332, text='push-over'), Row(id=30332, text='burying'), Row(id=30332, text='Bullet Voting'), Row(id=30332, text='Compromising'), Row(id=28737250, text='ordinal preferences'), Row(id=30332, text='Voting strategy'), Row(id=1133911, text='Condorcet winner'), Row(id=2111299, text='Condorcet loser'), Row(id=51993810, text='dichotomous preferences'), Row(id=886330, text='strategy-proof'), Row(id=1133911, text='Condorcet criterion'), Row(id=6732187, text='later-no-harm criterion'), Row(id=258957, text='monotonicity criterion'), Row(id=26898094, text='von Neumann–Morgenstern utility theorem'), Row(id=30332, text='rational voter model'), Row(id=13737627, text='Myerson'), Row(id=45479, text='expected utility'), Row(id=2111299, text='Condorcet loser'), Row(id=44446, text='here'), Row(id=11924, text='game theory'), Row(id=20169599, text='Unrestricted domain'), Row(id=20169520, text='Non-dictatorship'), Row(id=45468, text='Pareto efficiency'), Row(id=89425, text=\"Arrow's impossibility theorem\"), Row(id=1725354, text='Majority criterion'), Row(id=258957, text='Monotonicity criterion'), Row(id=1792861, text='Consistency criterion'), Row(id=1133956, text='Participation criterion'), Row(id=1133911, text='Condorcet criterion'), Row(id=44446, text='pairwise comparison'), Row(id=2111299, text='Condorcet loser criterion'), Row(id=259105, text='Independence of irrelevant alternatives'), Row(id=6750188, text='Independence of clones criterion'), Row(id=2416710, text='Reversal symmetry'), Row(id=20169599, text='Unrestricted domain'), Row(id=20169520, text='Non-dictatorship'), Row(id=45468, text='Pareto efficiency'), Row(id=1725354, text='Majority'), Row(id=258957, text='Monotone'), Row(id=1792861, text='Consistency'), Row(id=1133956, text='Participation'), Row(id=1133911, text='Condorcet'), Row(id=2111299, text='Condorcet loser'), Row(id=259105, text='IIA'), Row(id=6750188, text='Clone independence'), Row(id=2416710, text='Reversal symmetry'), Row(id=17071161, text='Strong Nash equilibrium'), Row(id=89425, text=\"Arrow's impossibility theorem\"), Row(id=1908021, text='mutual majority criterion'), Row(id=1133921, text='Smith criterion'), Row(id=10880, text='plurality'), Row(id=295237, text='disapproval voting'), Row(id=129203, text='recall'), Row(id=10880, text='plurality voting'), Row(id=32217517, text='Majority Judgment'), Row(id=68033962, text='Multiwinner approval voting'), Row(id=68033962, text='Party-approval voting'), Row(id=25908, text='Score Voting'), Row(id=27905391, text='Big Think')])]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in this bucket in GCP we will store our indexes-files\n",
        "bucket_name = 'project_ir_inverted_index_test'"
      ],
      "metadata": {
        "id": "cGnYo9OP8-lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Document body inverted index**"
      ],
      "metadata": {
        "id": "xOvp0sXotIH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create small corpus of documents text"
      ],
      "metadata": {
        "id": "EOmtnK0vF_Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - change to unlimit when building the real corpus\n",
        "\n",
        "# take the selected parameters of the first 1,000 rows and create an RDD from it\n",
        "general_1000_docs_body = parquetFile.limit(1000).select(\"text\",\"id\").rdd\n",
        "#general_1000_docs_body.take(20)\n",
        "\n",
        "in_train_test_docs_body = df.select(\"text\",\"id\").rdd\n",
        "#in_train_test_docs.take(20)\n",
        "\n",
        "#RDD of our temporary corpus - delte when build the real index\n",
        "united_body_corpus = in_train_test_docs_body.union(general_1000_docs_body).distinct()\n",
        "#united.count()"
      ],
      "metadata": {
        "id": "_4zLZ4ClxLNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate **tf** of the docs in **'united_body_corpus'** RDD (docs body text)"
      ],
      "metadata": {
        "id": "4mtUXFjFvfjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts_body = united_body_corpus.flatMap(lambda x: word_count(x[0], x[1],True))"
      ],
      "metadata": {
        "id": "7vLBHV-qvUFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts_body.take(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E--8CbWAHrYD",
        "outputId": "75bed3b2-0512-4f2f-c003-5eb3a2b9c2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('right', (1857, 2)),\n",
              " ('approval', (1857, 154)),\n",
              " ('ballot', (1857, 16)),\n",
              " ('voter', (1857, 56))]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcaulte DL for body \n",
        "DL_body = united_body_corpus.map(lambda x: calculate_DL(x[0], x[1], True)).collectAsMap()"
      ],
      "metadata": {
        "id": "orF1wN_o6a-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_total_body = (word_counts_body.map(lambda x: (x[0], x[1][1]))).reduceByKey(lambda x, y: x + y).collectAsMap()"
      ],
      "metadata": {
        "id": "2rpEcR80HcXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out rare words, calculate DF, and write the complete index"
      ],
      "metadata": {
        "id": "9OZkbwmqx_4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - (when working on the entire corpus, we will increase this threshold to a minimum of 50 documents).\n",
        "postings_body = word_counts_body.groupByKey().mapValues(reduce_word_counts)\n",
        "postings_filtered_body = postings_body.filter(lambda x: len(x[1])>10)\n",
        "\n",
        "#RDD of (token, df) pair\n",
        "w2df_body = calculate_df(postings_filtered_body).collectAsMap()\n",
        "\n",
        "# todo delete this cell\n",
        "posting_locs_list_body = partition_postings_and_write(postings_filtered_body).collect()\n",
        "\n",
        "# merge the posting locations into a single dict and run more tests (5 points) - todo delete this cell\n",
        "super_posting_locs_body = defaultdict(list)\n",
        "for posting_loc in posting_locs_list_body:\n",
        "  for k, v in posting_loc.items():\n",
        "    super_posting_locs_body[k].extend(v)\n",
        "\n",
        "# Create inverted index instance\n",
        "bodyIndex = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "bodyIndex.posting_locs = super_posting_locs_body\n",
        "# Add the token - df dictionary to the inverted index\n",
        "bodyIndex.df = w2df_body\n",
        "# @TODO: update - added DL dict\n",
        "bodyIndex.DL = DL_body\n",
        "#@TODO: update - added DL dict\n",
        "bodyIndex.term_total = term_total_body\n",
        "# write the global stats out\n",
        "bodyIndex.write_index('.', 'bodyIndex') #todo - change name to index_body"
      ],
      "metadata": {
        "id": "EC1758M7x8yS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #write index to bucket in GCP\n",
        "# index_src = \"bodyIndex.pkl\"\n",
        "# index_dst = f'gs://{bucket_name}/index_gcp/{index_src}'\n",
        "# !gsutil cp $index_src $index_dst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9T8fwFE93CB",
        "outputId": "03caa027-083f-44b6-f5f3-b00a634ccf1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://bodyIndex.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][898.0 KiB/898.0 KiB]                                                \n",
            "Operation completed over 1 objects/898.0 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #make sure that the data was written\n",
        "# !gsutil ls -lh $index_dst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaojAAWy7_N-",
        "outputId": "0555efcd-8740-4cf9-a5d8-92c1c3bb8116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "898.04 KiB  2022-01-01T14:07:31Z  gs://project_ir_inverted_index_test/index_gcp/bodyIndex.pkl\n",
            "TOTAL: 1 objects, 919591 bytes (898.04 KiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Document title inverted index**"
      ],
      "metadata": {
        "id": "vqlg_TQjJhdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create small corpus of documents title"
      ],
      "metadata": {
        "id": "kBqr5XIGJ3ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - delete this section before submission\n",
        "\n",
        "# take the selected parameters of the first 1,000 rows and create an RDD from it\n",
        "general_1000_docs_title = parquetFile.limit(1000).select(\"title\",\"id\").rdd #todo - comment\n",
        "\n",
        "in_train_test_docs_title = df.select(\"title\",\"id\").rdd #todo - comment\n",
        "\n",
        "#RDD of our temporary corpus\n",
        "united_title_corpus = in_train_test_docs_title.union(general_1000_docs_title).distinct()"
      ],
      "metadata": {
        "id": "5lQZRjwTJu0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate **tf** of the docs in **'united_title_corpus'** RDD (docs title text)"
      ],
      "metadata": {
        "id": "LzXc0tOXK_JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts_title = united_title_corpus.flatMap(lambda x: word_count(x[0], x[1],False))"
      ],
      "metadata": {
        "id": "vFexvqu_K_Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out rare words, calculate DF, and write the complete index"
      ],
      "metadata": {
        "id": "_cklqF4eK_Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - (when working on the entire corpus, we will increase this threshold to a minimum of 50 documents).\n",
        "postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n",
        "#RDD of (token, df) pair\n",
        "w2df_title = calculate_df(postings_title).collectAsMap()\n",
        "\n",
        "posting_locs_list_title = partition_postings_and_write(postings_title).collect()\n",
        "\n",
        "# merge the posting locations into a single dict and run more tests (5 points) - todo delete this cell\n",
        "super_posting_locs_title = defaultdict(list)\n",
        "for posting_loc in posting_locs_list_title:\n",
        "  for k, v in posting_loc.items():\n",
        "    super_posting_locs_title[k].extend(v)\n",
        "\n",
        "# Create inverted index instance\n",
        "titleIndex = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "titleIndex.posting_locs = super_posting_locs_title\n",
        "# Add the token - df dictionary to the inverted index\n",
        "titleIndex.df = w2df_title\n",
        "# write the global stats out\n",
        "titleIndex.write_index('.', 'titleIndex') #todo - change name to index_body"
      ],
      "metadata": {
        "id": "JVLaqQU-K_Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #write index to bucket in GCP\n",
        "# index_src_title = \"titleIndex.pkl\"\n",
        "# index_dst_title = f'gs://{bucket_name}/index_gcp/{index_src_title}' #todo - write to other file?\n",
        "# !gsutil cp $index_src_title $index_dst_title\n",
        "\n",
        "# #make sure that the data was written\n",
        "# !gsutil ls -lh $index_dst_title"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhx_AQoXN9J5",
        "outputId": "e84728f9-7634-40fc-ad8b-cdd004cde4f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://titleIndex.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 74.4 KiB/ 74.4 KiB]                                                \n",
            "Operation completed over 1 objects/74.4 KiB.                                     \n",
            " 74.45 KiB  2022-01-01T15:57:44Z  gs://project_ir_inverted_index_test/index_gcp/titleIndex.pkl\n",
            "TOTAL: 1 objects, 76233 bytes (74.45 KiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Document anchor text inverted index**"
      ],
      "metadata": {
        "id": "PZs5PXsYQNRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create small corpus of documents anchor text"
      ],
      "metadata": {
        "id": "TeBgthFvQNRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - delete this section before submission\n",
        "\n",
        "# take the selected parameters of the first 1,000 rows and create an RDD from it\n",
        "general_1000_docs_anchor_text = parquetFile.limit(1000).select(\"id\",\"anchor_text\").rdd \n",
        "general_1000_docs_anchor_text_flat = general_1000_docs_anchor_text.map(lambda x : (x[0],\", \".join([y[\"text\"] for y in x[1]])))\n",
        "\n",
        "\n",
        "#general_1000_docs_anchor_text.take(20)\n",
        "\n",
        "in_train_test_docs_anchor_text = df.select(\"id\",\"anchor_text\").rdd #todo - comment\n",
        "in_train_test_docs_anchor_text_flat = in_train_test_docs_anchor_text.map(lambda x : (x[0],\", \".join([y[\"text\"] for y in x[1]])))\n",
        "\n",
        "#RDD of our temporary corpus\n",
        "\n",
        "united_anchor_text_corpus = in_train_test_docs_anchor_text_flat.union(general_1000_docs_anchor_text_flat).distinct()"
      ],
      "metadata": {
        "id": "Zc7RZ2SSQNRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate **tf** of the docs in **'united_title_corpus'** RDD (docs title text)"
      ],
      "metadata": {
        "id": "dhW29GzhQNRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AHXI7UB4Vi4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts_anchor_text = united_anchor_text_corpus.flatMap(lambda x: word_count(str(x[1]), x[0],True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSsKTL6uQNRi",
        "outputId": "a2ae3c8d-6958-473c-e747-bc586e48c7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('vernor', (2080, 2))]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out rare words, calculate DF, and write the complete index"
      ],
      "metadata": {
        "id": "1cIz6mumNQ8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - (when working on the entire corpus, we will increase this threshold to a minimum of 50 documents).\n",
        "postings_anchor_text = word_counts_anchor_text.groupByKey().mapValues(reduce_word_counts)\n",
        "#RDD of (token, df) pair\n",
        "w2df_anchor_text = calculate_df(postings_anchor_text).collectAsMap()\n",
        "\n",
        "posting_locs_list_anchor_text = partition_postings_and_write(postings_anchor_text).collect()\n",
        "\n",
        "# merge the posting locations into a single dict and run more tests (5 points) - todo delete this cell\n",
        "super_posting_locs_anchor_text = defaultdict(list)\n",
        "for posting_loc in posting_locs_list_anchor_text:\n",
        "  for k, v in posting_loc.items():\n",
        "    super_posting_locs_anchor_text[k].extend(v)\n",
        "\n",
        "# Create inverted index instance\n",
        "anchorTextIndex = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "anchorTextIndex.posting_locs = super_posting_locs_anchor_text\n",
        "# Add the token - df dictionary to the inverted index\n",
        "anchorTextIndex.df = w2df_anchor_text\n",
        "# write the global stats out\n",
        "anchorTextIndex.write_index('.', 'anchorTextIndex') #todo - change name to index_body"
      ],
      "metadata": {
        "id": "MHw_ffNAQNRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write index to bucket in GCP\n",
        "# index_src_anchorText = \"anchorTextIndex.pkl\"\n",
        "# index_dst_anchorText = f'gs://{bucket_name}/index_gcp/{index_src_anchorText}' #todo - write to other file?\n",
        "# !gsutil cp $index_src_anchorText $index_dst_anchorText\n",
        "\n",
        "# #make sure that the data was written\n",
        "# !gsutil ls -lh $index_dst_anchorText\n",
        "\n",
        "#https://console.cloud.google.com/storage/browser/project_ir_inverted_index_test/index_gcp;tab=objects?authuser=1&project=ir-course-334114&pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m84pMqb1OSum",
        "outputId": "6cc0842d-2e59-4482-a95d-2081207a26ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://anchorTextIndex.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  4.9 MiB/  4.9 MiB]                                                \n",
            "Operation completed over 1 objects/4.9 MiB.                                      \n",
            "  4.93 MiB  2022-01-01T15:58:26Z  gs://project_ir_inverted_index_test/index_gcp/anchorTextIndex.pkl\n",
            "TOTAL: 1 objects, 5174238 bytes (4.93 MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sLv4_ZXMd1xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating similarity on bodyIndex"
      ],
      "metadata": {
        "id": "JvJZ8mwhd5oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_posting_gen(index):\n",
        "    \"\"\"\n",
        "    This function returning the generator working with posting list.\n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    index: inverted index    \n",
        "    \"\"\"\n",
        "    words, pls = zip(*index.posting_lists_iter())\n",
        "    return words,pls"
      ],
      "metadata": {
        "id": "lTsXOSnjeLdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_body, pls_body = zip(*bodyIndex.posting_lists_iter())\n",
        "#plurality"
      ],
      "metadata": {
        "id": "b2dv3h1Ce7_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_body['plurality'], pls_body"
      ],
      "metadata": {
        "id": "jjuK7UHzwWG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbPJ5YxcXRcT"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def generate_query_tfidf_vector(query_to_search,index):\n",
        "    \"\"\" \n",
        "    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n",
        "    The terms representing the query will be the unique terms in the index.\n",
        "\n",
        "    We will use tfidf on the query as well. \n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the query.    \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    vectorized query with tfidf scores\n",
        "    \"\"\"\n",
        "    \n",
        "    epsilon = .0000001\n",
        "    total_vocab_size = len(index.term_total)\n",
        "    Q = np.zeros((total_vocab_size))\n",
        "    term_vector = list(index.term_total.keys())    \n",
        "    counter = Counter(query_to_search)\n",
        "    for token in np.unique(query_to_search):\n",
        "        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               \n",
        "            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query\n",
        "            df = index.df[token]            \n",
        "            idf = math.log((len(index.DL))/(df+epsilon),10) #smoothing\n",
        "            \n",
        "            try:\n",
        "                ind = term_vector.index(token)\n",
        "                Q[ind] = tf*idf                    \n",
        "            except:\n",
        "                pass\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec_q = generate_query_tfidf_vector([\"plurality\"], bodyIndex)"
      ],
      "metadata": {
        "id": "diK4x_sYhe-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(vec_q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS2DdSKNhweS",
        "outputId": "e8db2201-620c-4a5e-e0c5-4323de738e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhQRw9ye10r1"
      },
      "outputs": [],
      "source": [
        "def get_candidate_documents_and_scores(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n",
        "    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n",
        "    Then it will populate the dictionary 'candidates.'\n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the document.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: generator for working with posting.\n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of candidates. In the following format:\n",
        "                                                               key: pair (doc_id,term)\n",
        "                                                               value: tfidf score. \n",
        "    \"\"\"\n",
        "    candidates = {}\n",
        "    N = len(index.DL)        \n",
        "    for term in np.unique(query_to_search):        \n",
        "        if term in words:            \n",
        "            list_of_doc = pls[words.index(term)]\n",
        "            # update - Removed str function in DL[doc_id] | normlized_tfidf = [(doc_id,(freq/index.DL[str(doc_id)])*math.log(N/index.df[term],10)) for doc_id, freq in list_of_doc]           \n",
        "            normlized_tfidf = [(doc_id,(freq/index.DL[doc_id])*math.log(N/index.df[term],10)) for doc_id, freq in list_of_doc]           \n",
        "                        \n",
        "            for doc_id, tfidf in normlized_tfidf:\n",
        "                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n",
        "        \n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list_of_doc = pls_body[words_body.index(\"plurality\")]                        \n",
        "# print(list_of_doc)\n",
        "# bodyIndex.DL[324]\n",
        "candidates = get_candidate_documents_and_scores([\"plurality\"],bodyIndex,words_body,pls_body)"
      ],
      "metadata": {
        "id": "DBwCvFVSqeal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQZF_xstDhm",
        "outputId": "73c8a349-3dde-45dc-df02-4c28e92729e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(324, 'plurality'): 0.0004193107049678477,\n",
              " (700, 'plurality'): 0.0002503280414438276,\n",
              " (1168, 'plurality'): 0.0007304333468869153,\n",
              " (1583, 'plurality'): 0.0009460498740945465,\n",
              " (1623, 'plurality'): 0.0005386130012501382,\n",
              " (1624, 'plurality'): 0.000233994802922571,\n",
              " (1857, 'plurality'): 0.008056137671300957,\n",
              " (2154, 'plurality'): 0.00022256682565059314,\n",
              " (2185, 'plurality'): 0.00023609407396218464,\n",
              " (2218, 'plurality'): 0.0010901099774426658,\n",
              " (7959, 'plurality'): 0.00023660231750496764,\n",
              " (23298, 'plurality'): 0.007546514967763548,\n",
              " (29589, 'plurality'): 0.0022124908245550376,\n",
              " (30332, 'plurality'): 0.007136022283908578,\n",
              " (31664, 'plurality'): 0.0007461855037287257}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11pKF-MqFhAt"
      },
      "outputs": [],
      "source": [
        "def generate_document_tfidf_matrix(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a DataFrame `D` of tfidf scores for a given query. \n",
        "    Rows will be the documents candidates for a given query\n",
        "    Columns will be the unique terms in the index.\n",
        "    The value for a given document and term will be its tfidf score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: generator for working with posting.\n",
        "    Returns:\n",
        "    -----------\n",
        "    DataFrame of tfidf scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    total_vocab_size = len(index.term_total)\n",
        "    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n",
        "    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n",
        "    D = np.zeros((len(unique_candidates), total_vocab_size))\n",
        "    D = pd.DataFrame(D)\n",
        "    \n",
        "    D.index = unique_candidates\n",
        "    D.columns = index.term_total.keys()\n",
        "\n",
        "    for key in candidates_scores:\n",
        "        tfidf = candidates_scores[key]\n",
        "        doc_id, term = key    \n",
        "        D.loc[doc_id][term] = tfidf\n",
        "\n",
        "    return D\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "92OO-c4Ah7TJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3334e351e5f2e85d04687f6640f4aa37",
          "grade": false,
          "grade_id": "cell-fab214a9b44a8c5e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(D,Q):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "    key: doc_id\n",
        "    value: cosine similarity score\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    D: DataFrame of tfidf scores.\n",
        "\n",
        "    Q: vectorized query with tfidf scores\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of cosine similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: cosine similarty score.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    df_cosine_similarity = defaultdict(int)\n",
        "    q_len = len(Q)\n",
        "    for index, row in D.iterrows():\n",
        "      di_len = len(row)\n",
        "      numerator = np.dot(row,Q)\n",
        "      denominator = q_len*di_len\n",
        "      df_cosine_similarity[index] = (numerator/denominator)\n",
        "    return df_cosine_similarity\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh7H2unw9oTf"
      },
      "outputs": [],
      "source": [
        "def get_top_n(sim_dict,N=3):\n",
        "    \"\"\" \n",
        "    Sort and return the highest N documents according to the cosine similarity score.\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "   \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sim_dict: a dictionary of similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n",
        "\n",
        "    N: Integer (how many documents to retrieve). By default N = 3\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    a ranked list of pairs (doc_id, score) in the length of N.\n",
        "    \"\"\"\n",
        "    \n",
        "    return sorted([(doc_id,builtins.round(score,5)) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)[:N]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "2AJ1qn2YVpN5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "14f6615a5239e4111808f84386c96e0a",
          "grade": false,
          "grade_id": "cell-3d601abdd80ad4b0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_topN_score_for_queries(query,index,N=3):\n",
        "    \"\"\" \n",
        "    Generate a dictionary that gathers for every query its topN score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    queries_to_search: a dictionary of queries as follows: \n",
        "                                                        key: query_id\n",
        "                                                        value: list of tokens.\n",
        "    index:           inverted index loaded from the corresponding files.    \n",
        "    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    return: a dictionary of queries and topN pairs as follows:\n",
        "                                                        key: query_id\n",
        "                                                        value: list of pairs in the following format:(doc_id, score). \n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    vec_q = generate_query_tfidf_vector(query, index)\n",
        "    mat_tfidf = generate_document_tfidf_matrix(query,index,words_body,pls_body)\n",
        "    sim_dict = cosine_similarity(mat_tfidf,vec_q)\n",
        "    return get_top_n(sim_dict, N)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_queries_score_train = get_topN_score_for_queries('plurality', bodyIndex)"
      ],
      "metadata": {
        "id": "4vyItTNnxIgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_queries_score_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Zbs0KW0drv",
        "outputId": "91607a4d-77c9-4b59-c458-862190a3dcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(324, 0.0), (700, 0.0), (1168, 0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}