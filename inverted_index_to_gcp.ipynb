{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naama133/WIKI_IR_ENGINE/blob/master/inverted_index_to_gcp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will create our inverted indices, by reading the wiki dump file from a gcp bucket.\n",
        "Our indices will be saved in our GCP bucket storage, and will be used for our search engine.\n",
        "\n",
        "We will create 3 instances of inverted index: one for the title, one for the body and one for the anchor text.\n",
        "\n",
        "### Setup - General imports:"
      ],
      "metadata": {
        "id": "fFkqlrD1lPYL"
      },
      "id": "fFkqlrD1lPYL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc60041",
      "metadata": {
        "id": "cfc60041"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing, importing, and initializing PySpark"
      ],
      "metadata": {
        "id": "_gIfFF5plgav"
      },
      "id": "_gIfFF5plgav"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "170fe92f",
      "metadata": {
        "id": "170fe92f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy wiki data"
      ],
      "metadata": {
        "id": "hBO6QehSljev"
      },
      "id": "hBO6QehSljev"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52de17d4",
      "metadata": {
        "id": "52de17d4",
        "outputId": "510692f7-d803-4794-d96f-f008cf886f4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "full_path = \"gs://wikidata_preprocessed/*\"\n",
        "parquetFile = spark.read.parquet(full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3249704",
      "metadata": {
        "id": "e3249704",
        "outputId": "495c48dd-2567-4506-91cf-037440df215c",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inverted_index_gcp.py\r\n"
          ]
        }
      ],
      "source": [
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea57a638",
      "metadata": {
        "id": "ea57a638"
      },
      "outputs": [],
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inverted_index_gcp import InvertedIndex"
      ],
      "metadata": {
        "id": "wy3iTvxt2Mkw"
      },
      "id": "wy3iTvxt2Mkw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13eabea6",
      "metadata": {
        "id": "13eabea6"
      },
      "outputs": [],
      "source": [
        "bucket_name = \"project_ir_test\"\n",
        "client = storage.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create stopwords RDD"
      ],
      "metadata": {
        "id": "Pb6CgJ03lp7b"
      },
      "id": "Pb6CgJ03lp7b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a9ff73",
      "metadata": {
        "id": "50a9ff73"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **function for all of the 3 indeces:**"
      ],
      "metadata": {
        "id": "EHqtzVrPlzCP"
      },
      "id": "EHqtzVrPlzCP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a21333a",
      "metadata": {
        "id": "0a21333a"
      },
      "outputs": [],
      "source": [
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def tokenize(text, removeStopword):\n",
        "  '''\n",
        "  Returns list of tokens after tokenize the given text.\n",
        "  choose if we want to remove stopword by using removeStopword boolean argument.\n",
        "  '''\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  if removeStopword:\n",
        "    tokens = [token for token in tokens if token not in all_stopwords]\n",
        "  return tokens\n",
        "    \n",
        "def word_count(text, id, removeStopword):\n",
        "  '''\n",
        "  Count the frequency of each word in the given text (tf),\n",
        "  and choose if we want to remove stopword by using removeStopword boolean argument.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "    text: str: Text/title/anchor text of one document\n",
        "    id: int: Document id\n",
        "  Returns:\n",
        "  --------\n",
        "    A list of (token, (doc_id, tf)) pairs \n",
        "  '''\n",
        "  tokens = tokenize(text, removeStopword)\n",
        "  token_counter = Counter(tokens)\n",
        "  tokens_wo_dup = []\n",
        "  for token in tokens:\n",
        "     if token not in tokens_wo_dup:\n",
        "       tokens_wo_dup.append(token)\n",
        "  return [(token, (id, token_counter[token])) for token in tokens_wo_dup]\n",
        "\n",
        "\n",
        "def doc_to_term_counter(text, removeStopword):\n",
        "  '''\n",
        "  Calculates word counter for a given document\n",
        "  '''\n",
        "  tokens = tokenize(text, removeStopword)\n",
        "  token_counter = Counter(tokens)\n",
        "  return token_counter\n",
        "  \n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  '''\n",
        "  Gets a list of values (unsorted posting list) and returns a sorted list (sorted posting list by wiki_id)\n",
        "  (Operates on the pairs returned by word_count)\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "    unsorted_pl: A list of (wiki_id, tf) tuples \n",
        "  Returns:\n",
        "  --------\n",
        "    A sorted posting list.\n",
        "  '''\n",
        "  return sorted(unsorted_pl,key=lambda x: x[0])\n",
        "\n",
        "def calculate_df(postings):\n",
        "  ''' Takes a posting list RDD and calculate the df for each token.\n",
        "  Parameters: Postings is an RDD where each element is a (token, posting_list) pair.\n",
        "  Returns: An RDD where each element is a (token, df) pair.\n",
        "  '''\n",
        "  return postings.groupByKey().mapValues(lambda x: len(list(x)[0]))\n",
        "\n",
        "def partition_postings_and_write(postings, storage_path):\n",
        "  '''\n",
        "  partitions the posting list, writes out each bucket,\n",
        "  and returns information about the location on storage of each posting list.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "    postings: an RDD where each item is a (w, posting_list) pair.\n",
        "    storage_path: where to write in storage \n",
        "  Returns: an RDD where each item is a posting locations dictionary for a bucket.\n",
        "  '''\n",
        "  return (\n",
        "      postings\n",
        "      .map(lambda x: (token2bucket_id(x[0]),(x[0],x[1])))\n",
        "      .groupByKey()\n",
        "      .map(lambda x: InvertedIndex.write_a_posting_list(x, bucket_name, storage_path)))\n",
        "\n",
        "def calculate_DL(text, id, removeStopword):\n",
        "  '''\n",
        "  calculate docs len. returns a tuple of (id, doc len)\n",
        "  choose if we want to remove stopword by using removeStopword boolean argument.\n",
        "  '''\n",
        "  tokens = tokenize(text, removeStopword)\n",
        "  return((id,len(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73344cc0",
      "metadata": {
        "id": "73344cc0"
      },
      "outputs": [],
      "source": [
        "def createInvertedIndexInstance(super_posting_locs, w2df_dict, DL_dic, term_total, doc_id_to_norm, avgDl, pathName, bucket, needUpdateDocTitleDic):\n",
        "  '''\n",
        "  Create inverted index instance & write it to GCP function\n",
        "  '''\n",
        "  inverted_index = InvertedIndex()\n",
        "\n",
        "  # Adding the posting locations dictionary to the inverted index\n",
        "  inverted_index.posting_locs = super_posting_locs\n",
        "\n",
        "  # Add the token - df dictionary to the inverted index\n",
        "  inverted_index.df = w2df_dict\n",
        "\n",
        "  # Added DL_body dict\n",
        "  inverted_index.DL = DL_dic\n",
        "\n",
        "  # Added term_total dict\n",
        "  inverted_index.term_total = term_total\n",
        "\n",
        "  inverted_index.total_vec_size = len(inverted_index.term_total)\n",
        "\n",
        "  inverted_index.doc_id_to_norm = doc_id_to_norm\n",
        "\n",
        "  inverted_index.avgDl = avgDl # avg documents length in the index\n",
        "\n",
        "  if needUpdateDocTitleDic:\n",
        "    inverted_index.doc_id_to_title = united_title_corpus.collectAsMap()\n",
        "\n",
        "  # write the global stats out\n",
        "  inverted_index.write_index('.', pathName)\n",
        "\n",
        "  # upload to gs\n",
        "  index_src = pathName+\".pkl\"\n",
        "  index_dst = f'gs://{bucket}/postings_gcp/{pathName}/{index_src}'\n",
        "  !gsutil cp $index_src $index_dst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this point, we will strat to build 3 inverted indeces, one for the body, one for the title, and one for the anchor text"
      ],
      "metadata": {
        "id": "i5xCk8VZlvn6"
      },
      "id": "i5xCk8VZlvn6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Document body inverted index**"
      ],
      "metadata": {
        "id": "OFVSh0xoqEUn"
      },
      "id": "OFVSh0xoqEUn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d970345b",
      "metadata": {
        "id": "d970345b",
        "outputId": "a4502009-a219-4df7-9188-c1912dad885f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:======================================================>(122 + 2) / 124]\r"
          ]
        }
      ],
      "source": [
        "united_body_corpus = parquetFile.select(\"text\",\"id\").rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae6f281",
      "metadata": {
        "id": "1ae6f281",
        "outputId": "f4ca8c90-2f1b-4bfa-daf0-b6000379d6c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "word_counts_body = united_body_corpus.flatMap(lambda x: word_count(x[0], x[1],True))\n",
        "\n",
        "postings_body = word_counts_body.groupByKey().mapValues(reduce_word_counts)\n",
        "\n",
        "postings_filtered_body = postings_body.filter(lambda x: len(x[1])>50)\n",
        "\n",
        "w2df_dict_body = calculate_df(postings_filtered_body).collectAsMap()\n",
        "\n",
        "DL_body_rdd = united_body_corpus.map(lambda x: calculate_DL(x[0], x[1], True))\n",
        "\n",
        "avgDl_body = DL_body_rdd.map(lambda x: x[1]).mean()\n",
        "\n",
        "DL_body = DL_body_rdd.collectAsMap()\n",
        "\n",
        "term_total_body = (word_counts_body.map(lambda x: (x[0], x[1][1]))).reduceByKey(lambda x, y: x + y).collectAsMap()\n",
        "\n",
        "N = len(DL_body)\n",
        "\n",
        "doc_id_to_token_counter_body = united_body_corpus.map(lambda x: (x[1], doc_to_term_counter(x[0], True)))\n",
        "\n",
        "doc_id_to_norm_body = doc_id_to_token_counter_body.map(lambda x: (x[0], np.linalg.norm([(x[1][term] / DL_body[x[0]]) * math.log(N / w2df_dict_body.get(term, N), 10) for term in x[1]]))).collectAsMap()\n",
        "\n",
        "_ = partition_postings_and_write(postings_filtered_body, \"index_body\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af5f9dc",
      "metadata": {
        "id": "9af5f9dc"
      },
      "outputs": [],
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs_body = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/index_body'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs_body[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "200b96b1",
      "metadata": {
        "id": "200b96b1",
        "outputId": "11ede8da-a8ae-48e5-e7f3-6134b6a33f19",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://bodyIndex.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  2.8 MiB/  2.8 MiB]                                                \n",
            "Operation completed over 1 objects/2.8 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "#create the inverted index instance, and write it to GCP\n",
        "createInvertedIndexInstance(super_posting_locs_body, w2df_dict_body, DL_body, term_total_body, doc_id_to_norm_body, avgDl_body ,'index_body', bucket_name,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Document title inverted index**"
      ],
      "metadata": {
        "id": "ulBaGZgHv0YY"
      },
      "id": "ulBaGZgHv0YY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "a4502009-a219-4df7-9188-c1912dad885f",
        "id": "XYSSDMNNwAEH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:======================================================>(122 + 2) / 124]\r"
          ]
        }
      ],
      "source": [
        "united_title_corpus = parquetFile.select(\"id\",\"title\").rdd"
      ],
      "id": "XYSSDMNNwAEH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "f4ca8c90-2f1b-4bfa-daf0-b6000379d6c1",
        "id": "BvIOUZ5UwAES"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "word_counts_title = united_title_corpus.flatMap(lambda x: word_count(x[1], x[0],False))\n",
        "\n",
        "postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n",
        "\n",
        "w2df_dict_title = calculate_df(postings_title).collectAsMap()\n",
        "\n",
        "DL_title_rdd = united_title_corpus.map(lambda x: calculate_DL(x[1], x[0], False))\n",
        "\n",
        "avgDl_title = DL_title_rdd.map(lambda x: x[1]).mean()\n",
        "\n",
        "DL_title = DL_title_rdd.collectAsMap()\n",
        "\n",
        "term_total_title = (word_counts_title.map(lambda x: (x[0], x[1][1]))).reduceByKey(lambda x, y: x + y).collectAsMap()\n",
        "\n",
        "N = len(DL_title)\n",
        "\n",
        "doc_id_to_token_counter_title = united_title_corpus.map(lambda x: (x[0], doc_to_term_counter(x[1], True)))\n",
        "\n",
        "doc_id_to_norm_title = doc_id_to_token_counter_title.map(lambda x: (x[0], np.linalg.norm([(x[1][term] / DL_title[x[0]]) * math.log(N / w2df_dict_title.get(term, N), 10) for term in x[1]]))).collectAsMap()\n",
        "\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_title, \"index_title\").collect()"
      ],
      "id": "BvIOUZ5UwAES"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEf6yiBnwAES"
      },
      "outputs": [],
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs_title = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/index_title'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs_title[k].extend(v)"
      ],
      "id": "FEf6yiBnwAES"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "11ede8da-a8ae-48e5-e7f3-6134b6a33f19",
        "collapsed": true,
        "id": "vUgccHSlwAES"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://bodyIndex.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  2.8 MiB/  2.8 MiB]                                                \n",
            "Operation completed over 1 objects/2.8 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "#create the inverted index instance, and write it to GCP\n",
        "createInvertedIndexInstance(super_posting_locs_title, w2df_dict_title, DL_title, term_total_title, doc_id_to_norm_title, avgDl_title, 'index_title', bucket_name, True)"
      ],
      "id": "vUgccHSlwAES"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Document anchor text inverted index**"
      ],
      "metadata": {
        "id": "jsW26xkP9lDj"
      },
      "id": "jsW26xkP9lDj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "a4502009-a219-4df7-9188-c1912dad885f",
        "id": "XwQJlU3U9oYF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:======================================================>(122 + 2) / 124]\r"
          ]
        }
      ],
      "source": [
        "general_docs_anchor_text = parquetFile.select(\"id\",\"anchor_text\").rdd \n",
        "\n",
        "# Pointed documents RDD\n",
        "united_anchor_text_corpus = general_docs_anchor_text.flatMap(lambda x :x[1]).groupByKey().mapValues(list).map(lambda x : (x[0],\" \".join([y for y in x[1]])))"
      ],
      "id": "XwQJlU3U9oYF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "f4ca8c90-2f1b-4bfa-daf0-b6000379d6c1",
        "id": "Yd4FppiT9oYG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "word_counts_anchor_text = united_anchor_text_corpus.flatMap(lambda x: word_count(str(x[1]), x[0],True))\n",
        "\n",
        "postings_anchor_text = word_counts_anchor_text.groupByKey().mapValues(reduce_word_counts)\n",
        "\n",
        "w2df_anchor_text = calculate_df(postings_anchor_text).collectAsMap()\n",
        "\n",
        "DL_anchor_text_rdd = united_anchor_text_corpus.map(lambda x: calculate_DL(str(x[1]), x[0], True))\n",
        "\n",
        "avgDl_anchor_text = DL_anchor_text_rdd.map(lambda x: x[1]).mean()\n",
        "\n",
        "DL_anchor_text = DL_anchor_text_rdd.collectAsMap()\n",
        "\n",
        "term_total_anchor_text = (word_counts_anchor_text.map(lambda x: (x[0], x[1][1]))).reduceByKey(lambda x, y: x + y).collectAsMap()\n",
        "\n",
        "N = len(DL_anchor_text)\n",
        "\n",
        "doc_id_to_token_counter_anchor_text = united_anchor_text_corpus.map(lambda x: (x[0], doc_to_term_counter(x[1], True)))\n",
        "\n",
        "remove_doc_len_0 = doc_id_to_token_counter_anchor_text.filter(lambda x: DL_anchor_text[x[0]] != 0)\n",
        "\n",
        "doc_id_to_norm_anchor_text = remove_doc_len_0.map(lambda x: (x[0], np.linalg.norm([(x[1][term] / DL_anchor_text.get(x[0],1)) * math.log(N / w2df_anchor_text.get(term, N), 10) for term in x[1]]))).collectAsMap()\n",
        "\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_anchor_text, \"index_anchor_text\").collect()"
      ],
      "id": "Yd4FppiT9oYG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLma6qNG9oYH"
      },
      "outputs": [],
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs_title_anchor_text = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/index_anchor_text'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs_title_anchor_text[k].extend(v)"
      ],
      "id": "TLma6qNG9oYH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "11ede8da-a8ae-48e5-e7f3-6134b6a33f19",
        "collapsed": true,
        "id": "uMMSunMF9oYH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://bodyIndex.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  2.8 MiB/  2.8 MiB]                                                \n",
            "Operation completed over 1 objects/2.8 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "#create the inverted index instance, and write it to GCP\n",
        "createInvertedIndexInstance(super_posting_locs_title_anchor_text, w2df_anchor_text, DL_anchor_text, term_total_anchor_text, doc_id_to_norm_anchor_text, avgDl_anchor_text, 'index_anchor_text', bucket_name, False)"
      ],
      "id": "uMMSunMF9oYH"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "inverted_index_to_gcp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}